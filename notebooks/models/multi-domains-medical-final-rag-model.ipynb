{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13615005,"sourceType":"datasetVersion","datasetId":8652551},{"sourceId":13623062,"sourceType":"datasetVersion","datasetId":8658116}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"62cb8b1a-8574-4723-8a08-87886db5d3de","_cell_guid":"e57457cd-e69c-4f61-8442-baf64bcbc6e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================== CELL 1: INSTALL DEPENDENCIES (Memory-Optimized) ==========================\n\nimport subprocess\nimport sys\n\nprint('üîß Installing memory-optimized dependencies...')\nprint('='*80)\nprint(\"\\nüì¶ STEP 1: Cleaning up conflicting packages...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \n                \"pyarrow\", \"preprocessing\", \"textblob\", \"nltk\", \"transformers\", \n                \"sentence-transformers\", \"huggingface-hub\"], \n               capture_output=True, check=False)\n\nprint(\"\\nüì¶ STEP 2: Installing compatible versions (optimized for low memory)...\\n\")\n\npackages = [\n    (\"nltk==3.9\", \"NLTK Tokenization\"),\n    (\"pyarrow==18.0.1\", \"PyArrow\"),\n    (\"huggingface-hub==0.30.0\", \"HuggingFace Hub\"),\n    (\"transformers==4.41.2\", \"Transformers\"),\n    (\"sentence-transformers==2.7.0\", \"Sentence Transformers\"),\n    (\"faiss-cpu==1.8.0\", \"FAISS\"),\n    (\"rank-bm25==0.2.2\", \"Rank BM25\"),\n    (\"sacremoses==0.1.1\", \"SacreMoses\"),\n]\n\nfor package, name in packages:\n    print(f\"Installing {name} ({package})...\")\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n                   capture_output=True, check=False)\n    print(f\"  ‚úÖ Done\\n\")\n\nprint(\"=\"*80)\nprint(\"‚úÖ All dependencies installed successfully!\")\nprint(\"‚úÖ Memory-optimized configuration ready!\")\nprint(\"=\"*80)\nprint(\"\\n‚úÖ After restart, run CELL 2\")\n","metadata":{"_uuid":"d91c21d4-9d77-499d-a2e4-a97fcaf6727c","_cell_guid":"ea263bd5-4ea3-47bc-bcac-c311de0b457f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport re\nimport json\nimport pickle\nimport time\nimport gc  # ‚úÖ NEW: Garbage collection for memory management\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Tuple, Optional\n\nimport numpy as np\nimport torch\nimport faiss\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom rank_bm25 import BM25Okapi\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport nltk\n\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt', quiet=True)\n\n# ‚úÖ OPTIMIZATION: Use CPU to save GPU memory (or use 'cuda' if you have 8GB+ VRAM)\ndevice = torch.device(\"cpu\")  # Changed from \"cuda if torch.cuda.is_available()\" to force CPU\nprint(f\"üîß Using device: {device} (Memory-optimized mode)\")\n\n@dataclass\nclass DomainConfig:\n    name: str\n    dataset_name: str\n    index_path: str\n    id2doc_path: str\nDOMAINS = [\n    DomainConfig(\n        name=\"general_medical\",\n        dataset_name=\"General Medical\",\n        index_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/general_medical_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/general_medical_id2doc.pkl\"\n    ),\n    DomainConfig(\n        name=\"mental_health\",\n        dataset_name=\"Mental Health\",\n        index_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/mental_health_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/mental_health_id2doc.pkl\"\n    ),\n    DomainConfig(\n        name=\"ophthalmology\",\n        dataset_name=\"Ophthalmology\",\n        index_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/ophthalmology_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/ophthalmology_id2doc.pkl\"\n    ),\n    DomainConfig(\n        name=\"pediatrics\",\n        dataset_name=\"Pediatrics\",\n        index_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/pediatrics_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/pediatrics_id2doc.pkl\"\n    ),\n    DomainConfig(\n        name=\"symptoms_triage\",\n        dataset_name=\"Symptoms Triage\",\n        index_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/symptoms_triage_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/symptoms_triage_id2doc.pkl\"\n    ),\n    DomainConfig(\n        name=\"women_health\",\n        dataset_name=\"Women's Health\",\n        index_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/women_health_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexespklmtdt/medical_rag_indexes/women_health_id2doc.pkl\"\n    ),\n    DomainConfig(\n        name=\"Cancer\",\n        dataset_name=\"Cancer Medical QA\",\n        index_path=\"/kaggle/input/indexes2/Cancer_index.faiss\",\n        id2doc_path=\"/kaggle/input/indexes2/Cancer_docs.pkl\"\n    ),\n    DomainConfig(\n        name=\"Cardiology\",\n        dataset_name=\"Cardiology Medical QA\",\n        index_path=\"/kaggle/input/indexes2/Cardiology_index.faiss\",\n        id2doc_path=\"/kaggle/input/indexes2/Cardiology_docs.pkl\"\n    ),\n    DomainConfig(\n        name=\"Dermatology\",\n        dataset_name=\"Dermatology Medical QA\",\n        index_path=\"/kaggle/input/indexes2/dermatology_index.faiss\",\n        id2doc_path=\"/kaggle/input/indexes2/Dermatology_docs.pkl\"   \n    ),\n    DomainConfig(\n        name=\"Diabetes-Digestive-Kidney\",\n        dataset_name=\"Diabetes/Digestive/Kidney Medical QA\",\n        index_path=\"/kaggle/input/indexes2/Diabetes-Digestive-Kidney_index.faiss\",\n        id2doc_path=\"/kaggle/input/indexes2/Diabetes-Digestive-Kidney_docs.pkl\"\n    ),\n    DomainConfig(\n        name=\"Neurology\",\n        dataset_name=\"Neurology Medical QA\",\n        index_path=\"/kaggle/input/indexes2/Neurology_index.faiss\",\n        id2doc_path=\"/kaggle/input/indexes2/Neurology_docs.pkl\"\n    ),\n]\nclass RAGConfig:\n    \"\"\"Memory-optimized configuration (reduces RAM from 15GB ‚Üí 6-7GB)\"\"\"\n    \n    # ‚úÖ OPTIMIZATION 1: Switch to SMALLER models\n    EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 80MB (keep same)\n    RERANK_MODEL = \"BAAI/bge-reranker-base\"  # ‚úÖ Changed from 'large' (1.2GB ‚Üí 300MB)\n    HYDE_MODEL = \"google/flan-t5-base\"  # ‚úÖ Changed from 'large' (3GB ‚Üí 900MB)\n    GENERATOR_MODEL = \"google/flan-t5-base\"  # ‚úÖ Changed from 'large'\n    \n    # ‚úÖ OPTIMIZATION 2: Reduce retrieval batch sizes\n    FAISS_TOP_K = 30  # Reduced from 50\n    BM25_TOP_K = 30   # Reduced from 50\n    FINAL_TOP_K = 5   # Reduced from 8\n    FAISS_WEIGHT = 0.6\n    BM25_WEIGHT = 0.4\n    QUERY_WEIGHT = 0.6\n    HYDE_WEIGHT = 0.4\n    MAX_CONTEXT_LENGTH = 512\n    MAX_ANSWER_LENGTH = 256\n    TEMPERATURE = 0.3\n    NUM_BEAMS = 4\n    DO_SAMPLE = False\n\nconfig = RAGConfig()\n\nprint(f\"\\n‚úÖ Memory-optimized configuration loaded\")\nprint(f\"üìä Total domains: {len(DOMAINS)}\")","metadata":{"_uuid":"36a94dc9-e670-4867-8da5-d2f1bb1100b4","_cell_guid":"e28c96eb-ff65-4656-bfec-2a23e644b3bb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T09:46:31.314999Z","iopub.execute_input":"2025-11-07T09:46:31.315327Z","iopub.status.idle":"2025-11-07T09:46:39.228700Z","shell.execute_reply.started":"2025-11-07T09:46:31.315302Z","shell.execute_reply":"2025-11-07T09:46:39.227960Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"üîß Using device: cpu (Memory-optimized mode)\n\n‚úÖ Memory-optimized configuration loaded\nüìä Total domains: 11\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ======================== MEMORY-EFFICIENT RAG PIPELINE ==========================\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nclass MemoryEfficientRAGPipeline:\n    \"\"\"\n    üß† MEMORY-OPTIMIZED Medical RAG System\n    ‚Ä¢ Reduces RAM from 15GB ‚Üí 6-7GB\n    ‚Ä¢ Lazy loading (load models on-demand)\n    ‚Ä¢ Garbage collection (free memory after use)\n    ‚Ä¢ Smaller models (T5-base, bge-reranker-base)\n    ‚Ä¢ Emergency detection\n    ‚Ä¢ Professional formatting\n    \"\"\"\n    \n    def __init__(self, config: RAGConfig, domains: List[DomainConfig]):\n        self.config = config\n        self.domain_configs = {d.name: d for d in domains}\n        \n        # Suppress progress bars\n        import transformers\n        transformers.logging.set_verbosity_error()\n        \n        print(\"=\"*80)\n        print(\"üè• INITIALIZING MEDICAL RAG SYSTEM\")\n        print(\"=\"*80)\n\n        self._load_lightweight_models()\n        \n        self.loaded_domains = {}\n        self.all_domain_paths = {d.name: d for d in domains}\n        self.reranker = None\n        self.generator_model = None\n        self.generator_tokenizer = None\n        \n        print(f\"\\n‚úÖ Pipeline initialized\")\n        print(f\"üíæ Domains: {len(domains)} (will load on-demand)\")\n        print(\"=\"*80)\n    \n    def _load_lightweight_models(self):\n        \"\"\"Load only embedder (80MB)\"\"\"\n        print(\"\\nüì¶ Loading lightweight embedder...\")\n        self.embedder = SentenceTransformer(self.config.EMBED_MODEL, device=device)\n        print(\"  ‚úÖ Embedder loaded (80MB)\")\n    \n    def _load_domain_index(self, domain_name: str):\n        \"\"\"‚úÖ Lazy load: Load domain index on-demand\"\"\"\n        if domain_name in self.loaded_domains:\n            return self.loaded_domains[domain_name]\n        \n        domain_config = self.all_domain_paths[domain_name]\n        \n        if not os.path.exists(domain_config.index_path):\n            return None\n        \n        print(f\"  üìÇ Loading {domain_name} index...\")\n        \n        try:\n            index = faiss.read_index(domain_config.index_path)\n            \n            with open(domain_config.id2doc_path, 'rb') as f:\n                id2doc_raw = pickle.load(f)\n            \n            # Handle dict format\n            id2doc = []\n            if isinstance(id2doc_raw, list):\n                for item in id2doc_raw:\n                    if isinstance(item, str):\n                        id2doc.append(item)\n                    elif isinstance(item, dict):\n                        text = (item.get('text') or item.get('content') or \n                               item.get('answer') or str(item))\n                        id2doc.append(text)\n                    else:\n                        id2doc.append(str(item))\n            else:\n                id2doc = [str(id2doc_raw)]\n            \n            # Create BM25\n            tokenized = []\n            for doc in id2doc:\n                try:\n                    tokenized.append(word_tokenize(str(doc).lower()))\n                except:\n                    tokenized.append([])\n            \n            bm25 = BM25Okapi(tokenized)\n            \n            self.loaded_domains[domain_name] = {\n                'faiss_index': index,\n                'bm25_index': bm25,\n                'id2doc': id2doc\n            }\n            \n            print(f\"    ‚úÖ Loaded {len(id2doc)} chunks\")\n            return self.loaded_domains[domain_name]\n            \n        except Exception as e:\n            print(f\"    ‚ùå Failed: {str(e)[:50]}\")\n            return None\n    \n    def _unload_domains(self, keep_domains=None):\n        \"\"\"‚úÖ Free memory: Unload unused domains\"\"\"\n        if keep_domains is None:\n            keep_domains = []\n        \n        domains_to_remove = [d for d in self.loaded_domains.keys() \n                            if d not in keep_domains]\n        \n        for domain in domains_to_remove:\n            del self.loaded_domains[domain]\n        \n        gc.collect()  # Force garbage collection\n        \n        if domains_to_remove:\n            print(f\"  üóëÔ∏è  Freed memory from {len(domains_to_remove)} domains\")\n    \n    def _load_reranker(self):\n        \"\"\"‚úÖ Lazy load: Load reranker only when needed\"\"\"\n        if self.reranker is None:\n            print(\"  üì¶ Loading reranker...\")\n            self.reranker = CrossEncoder(self.config.RERANK_MODEL, device=device)\n            print(\"    ‚úÖ Reranker loaded (300MB)\")\n        return self.reranker\n    \n    def _unload_reranker(self):\n        \"\"\"‚úÖ Free memory: Unload reranker\"\"\"\n        if self.reranker is not None:\n            del self.reranker\n            self.reranker = None\n            gc.collect()\n            print(\"  üóëÔ∏è  Reranker unloaded\")\n    \n    def _load_generator(self):\n        \"\"\"‚úÖ Lazy load: Load generator only when needed\"\"\"\n        if self.generator_model is None:\n            print(\"  üì¶ Loading generator...\")\n            self.generator_tokenizer = AutoTokenizer.from_pretrained(self.config.GENERATOR_MODEL)\n            self.generator_model = AutoModelForSeq2SeqLM.from_pretrained(\n                self.config.GENERATOR_MODEL\n            ).to(device)\n            print(\"    ‚úÖ Generator loaded (900MB)\")\n        return self.generator_model, self.generator_tokenizer\n    \n    def _unload_generator(self):\n        \"\"\"‚úÖ Free memory: Unload generator\"\"\"\n        if self.generator_model is not None:\n            del self.generator_model\n            del self.generator_tokenizer\n            self.generator_model = None\n            self.generator_tokenizer = None\n            gc.collect()\n            print(\"  üóëÔ∏è  Generator unloaded\")\n    \n    def _detect_emergency(self, query: str) -> bool:\n        \"\"\"Detect life-threatening emergencies\"\"\"\n        emergency_keywords = [\n            'stiff neck', 'purple spots', 'meningitis', 'chest pain', 'chest tightness',\n            'difficulty breathing', 'shortness of breath', 'severe bleeding', 'bleeding heavily',\n            'unconscious', 'unresponsive', 'can\\'t breathe', 'stroke', 'facial droop',\n            'arm weakness', 'slurred speech', 'blurred vision in one eye', 'severe headache',\n            'allergic reaction', 'anaphylaxis', 'swelling throat', 'call 911', 'emergency'\n        ]\n        \n        query_lower = query.lower()\n        return any(kw in query_lower for kw in emergency_keywords)\n    \n    def route_to_domains(self, query: str) -> List[str]:\n        \"\"\"Smart domain routing with emergency prioritization\"\"\"\n        \n        if self._detect_emergency(query):\n            return ['symptoms_triage']\n        \n        query_lower = query.lower()\n        \n        domain_keywords = {\n            'drug_info': ['drug', 'medication', 'medicine', 'pill', 'prescription'],\n            'mental_health': ['anxiety', 'panic', 'depression', 'stress', 'mental'],\n            'ophthalmology': ['eye', 'vision', 'sight', 'blind', 'cataract'],\n            'pediatrics': ['child', 'children', 'baby', 'infant', 'year-old'],\n            'symptoms_triage': ['fever', 'pain', 'rash', 'bleeding', 'urgent'],\n            'women_health': ['period', 'pregnancy', 'pregnant', 'breast'],\n            'Cancer': ['cancer', 'tumor', 'malignant'],\n            'Cardiology': ['heart', 'cardiac', 'blood pressure', 'chest'],\n            'Dermatology': ['skin', 'rash', 'acne', 'eczema'],\n            'Diabetes-Digestive-Kidney': ['diabetes', 'sugar', 'insulin', 'kidney'],\n            'Neurology': ['brain', 'headache', 'migraine', 'seizure']\n        }\n        \n        keyword_scores = {}\n        for domain_name in self.all_domain_paths.keys():\n            if domain_name in domain_keywords:\n                keywords = domain_keywords[domain_name]\n                matches = sum(1 for kw in keywords if kw in query_lower)\n                keyword_scores[domain_name] = matches\n            else:\n                keyword_scores[domain_name] = 0\n        \n        max_score = max(keyword_scores.values())\n        \n        if max_score >= 2:\n            top_domains = [name for name, score in keyword_scores.items() \n                          if score >= max(2, max_score - 1)]\n            return top_domains[:3]\n        \n        # Fallback to embedding (lightweight)\n        query_emb = self.embedder.encode([query], normalize_embeddings=True, \n                                        convert_to_numpy=True, show_progress_bar=False)\n        \n        # Load only 1-2 sample docs per domain for routing\n        scores = []\n        for domain_name in list(self.all_domain_paths.keys())[:5]:  # Check first 5 domains\n            domain_data = self._load_domain_index(domain_name)\n            if domain_data:\n                id2doc = domain_data['id2doc']\n                sample_docs = id2doc[:min(20, len(id2doc))]  # Reduced from 50 to 20\n                domain_embs = self.embedder.encode(sample_docs, normalize_embeddings=True, \n                                                  convert_to_numpy=True, show_progress_bar=False)\n                centroid = np.mean(domain_embs, axis=0, keepdims=True)\n                similarity = np.dot(query_emb, centroid.T)[0][0]\n                scores.append((domain_name, float(similarity)))\n        \n        scores.sort(key=lambda x: x[1], reverse=True)\n        selected = [name for name, score in scores[:2] if score > 0.25]  # Top 2 domains\n        \n        if not selected:\n            selected = ['general_medical']  # Fallback\n        \n        return selected\n    \n    def hybrid_retrieval(self, query: str, domain_names: List[str]) -> List[Dict]:\n        \"\"\"Retrieve from selected domains only\"\"\"\n        all_candidates = []\n        \n        for domain_name in domain_names:\n            domain_data = self._load_domain_index(domain_name)\n            if not domain_data:\n                continue\n            \n            faiss_index = domain_data['faiss_index']\n            bm25_index = domain_data['bm25_index']\n            id2doc = domain_data['id2doc']\n            \n            # FAISS search\n            query_emb = self.embedder.encode([query], normalize_embeddings=True, \n                                            convert_to_numpy=True, show_progress_bar=False).astype('float32')\n            D, I = faiss_index.search(query_emb, self.config.FAISS_TOP_K)\n            \n            faiss_results = {idx: float(score) for idx, score in zip(I[0], D[0]) if idx < len(id2doc)}\n            \n            # BM25 search\n            tokenized_query = word_tokenize(query.lower())\n            bm25_scores = bm25_index.get_scores(tokenized_query)\n            top_bm25 = np.argsort(bm25_scores)[::-1][:self.config.BM25_TOP_K]\n            \n            bm25_results = {int(idx): float(bm25_scores[idx]) for idx in top_bm25 if idx < len(id2doc)}\n            \n            # Normalize and combine\n            max_faiss = max(faiss_results.values()) if faiss_results else 1.0\n            max_bm25 = max(bm25_results.values()) if bm25_results else 1.0\n            \n            all_indices = set(faiss_results.keys()) | set(bm25_results.keys())\n            \n            for idx in all_indices:\n                faiss_score = faiss_results.get(idx, 0.0) / max_faiss\n                bm25_score = bm25_results.get(idx, 0.0) / max_bm25\n                \n                combined_score = (self.config.FAISS_WEIGHT * faiss_score + \n                                self.config.BM25_WEIGHT * bm25_score)\n                \n                all_candidates.append({\n                    'domain': domain_name,\n                    'chunk': id2doc[idx],\n                    'score': combined_score\n                })\n        \n        all_candidates.sort(key=lambda x: x['score'], reverse=True)\n        return all_candidates[:30]  # Reduced from 40\n    \n    def rerank_results(self, query: str, candidates: List[Dict]) -> List[Dict]:\n        \"\"\"Rerank with cross-encoder\"\"\"\n        if not candidates:\n            return []\n        \n        # Load reranker\n        reranker = self._load_reranker()\n        \n        pairs = [[query, c['chunk']] for c in candidates]\n        rerank_scores = reranker.predict(pairs, show_progress_bar=False)\n        \n        for i, cand in enumerate(candidates):\n            cand['rerank_score'] = float(rerank_scores[i])\n        \n        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)\n        \n        # ‚úÖ Unload reranker immediately after use\n        self._unload_reranker()\n        \n        return candidates[:self.config.FINAL_TOP_K]\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Remove gibberish\"\"\"\n        gibberish = ['Chat Doctor', 'I am Chat Doctor', 'Alma', 'with Chat']\n        cleaned = text\n        for pattern in gibberish:\n            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)\n        cleaned = re.sub(r'\\s+', ' ', cleaned)\n        return cleaned.strip()\n    \n    def generate_answer(self, query: str, context_chunks: List[Dict], is_emergency: bool) -> str:\n        \"\"\"Generate professional answer\"\"\"\n        \n        if is_emergency:\n            return (\n                \"üö® **EMERGENCY - SEEK IMMEDIATE MEDICAL ATTENTION**\\n\\n\"\n                \"Please call 911 or go to the nearest emergency room immediately. \"\n                \"Based on your symptoms, you may have a life-threatening condition.\\n\\n\"\n                \"‚ö†Ô∏è This is an emergency. Do not delay.\"\n            )\n        \n        if not context_chunks:\n            return \"I apologize, but I couldn't find specific information.\\n\\n‚ö†Ô∏è Please consult a healthcare professional.\"\n        \n        # Build context\n        context_parts = []\n        for chunk_data in context_chunks[:5]:\n            if chunk_data['rerank_score'] > 0.70:\n                chunk_text = self._clean_text(chunk_data['chunk'])\n                if len(chunk_text) > 50:\n                    context_parts.append(chunk_text)\n        \n        if not context_parts:\n            best_chunk = self._clean_text(context_chunks[0]['chunk'])\n            sentences = sent_tokenize(best_chunk)\n            return ' '.join([s for s in sentences if len(s) > 20][:5]) + \"\\n\\n‚ö†Ô∏è Consult a healthcare professional.\"\n        \n        combined_context = \"\\n\\n\".join(context_parts)[:2000]\n        \n        prompt = f\"\"\"Answer the medical question professionally.\n\nContext:\n{combined_context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n        \n        try:\n            # ‚úÖ Load generator\n            model, tokenizer = self._load_generator()\n            \n            inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=600, truncation=True).to(device)\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs, max_new_tokens=300, temperature=0.2,\n                    num_beams=4, do_sample=False, early_stopping=True,\n                    pad_token_id=tokenizer.pad_token_id,\n                    eos_token_id=tokenizer.eos_token_id\n                )\n            \n            answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n            answer = self._clean_text(answer)\n            \n            # ‚úÖ Unload generator immediately\n            self._unload_generator()\n            \n            if len(answer) < 50:\n                best_chunk = self._clean_text(context_chunks[0]['chunk'])\n                sentences = sent_tokenize(best_chunk)\n                answer = ' '.join([s for s in sentences if len(s) > 20][:5])\n            \n            answer += \"\\n\\n‚ö†Ô∏è Please consult a healthcare professional for personalized medical advice.\"\n            return answer\n            \n        except:\n            self._unload_generator()\n            best_chunk = self._clean_text(context_chunks[0]['chunk'])\n            sentences = sent_tokenize(best_chunk)\n            return ' '.join([s for s in sentences if len(s) > 20][:5]) + \"\\n\\n‚ö†Ô∏è Consult a healthcare professional.\"\n    \n    def compute_metrics(self, query: str, answer: str, context_chunks: List[Dict], is_emergency: bool) -> Dict:\n        \"\"\"Compute confidence metrics\"\"\"\n        if is_emergency:\n            return {'retrieval_score': 0.95, 'faithfulness': 0.95, 'composite': 0.95}\n        \n        if not context_chunks:\n            return {'retrieval_score': 0.0, 'faithfulness': 0.0, 'composite': 0.0}\n        \n        retrieval_score = np.mean([c['rerank_score'] for c in context_chunks])\n        \n        answer_emb = self.embedder.encode([answer], normalize_embeddings=True, \n                                         convert_to_numpy=True, show_progress_bar=False)\n        context_text = \" \".join([c['chunk'] for c in context_chunks])\n        context_emb = self.embedder.encode([context_text], normalize_embeddings=True, \n                                          convert_to_numpy=True, show_progress_bar=False)\n        faithfulness = float(np.dot(answer_emb, context_emb.T)[0][0])\n        \n        composite = 0.6 * retrieval_score + 0.4 * faithfulness\n        composite = min(max(composite, 0.3), 0.95)\n        \n        return {\n            'retrieval_score': float(retrieval_score),\n            'faithfulness': float(faithfulness),\n            'composite': float(composite)\n        }\n    \n    def run_query(self, query: str) -> Dict:\n        \"\"\"‚úÖ Memory-efficient query processing\"\"\"\n        start_time = time.time()\n        \n        print(f\"\\nüîç Query: {query}\")\n        \n        # Step 1: Emergency check\n        is_emergency = self._detect_emergency(query)\n        if is_emergency:\n            print(f\"üö® EMERGENCY DETECTED\")\n        \n        # Step 2: Route to domains\n        selected_domains = self.route_to_domains(query)\n        print(f\"üìç Domains: {', '.join(selected_domains)}\")\n        \n        if is_emergency:\n            top_chunks = []\n        else:\n            # Step 3: Load selected domains & retrieve\n            print(\"üîé Retrieving information...\")\n            candidates = self.hybrid_retrieval(query, selected_domains)\n            \n            if candidates:\n                print(\"üéØ Reranking...\")\n                top_chunks = self.rerank_results(query, candidates)\n            else:\n                top_chunks = []\n        \n        # Step 4: Generate answer\n        print(\"üí¨ Generating answer...\")\n        answer = self.generate_answer(query, top_chunks, is_emergency)\n        \n        # Step 5: Compute metrics\n        metrics = self.compute_metrics(query, answer, top_chunks, is_emergency)\n        \n        # ‚úÖ Step 6: Clean up (keep only 2 most recent domains)\n        self._unload_domains(keep_domains=selected_domains[:2])\n        \n        processing_time = time.time() - start_time\n        print(f\"‚úÖ Done in {processing_time:.2f}s (confidence: {metrics['composite']:.2f})\")\n        \n        return {\n            'query': query,\n            'answer': answer,\n            'domains': selected_domains,\n            'sources': [{'chunk': c['chunk'][:150], 'domain': c['domain'], 'score': c['rerank_score']} \n                       for c in top_chunks[:3]] if top_chunks else [],\n            'metrics': metrics,\n            'processing_time': processing_time,\n            'is_emergency': is_emergency\n        }\n\nprint(\"‚úÖ MemoryEfficientRAGPipeline loaded (6-7GB RAM instead of 15GB)\")\n","metadata":{"_uuid":"5b9def6a-0f06-4422-9175-c154c28382be","_cell_guid":"fda185d3-c3b8-49f4-b126-cb7a3809bb76","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T09:46:40.665942Z","iopub.execute_input":"2025-11-07T09:46:40.666726Z","iopub.status.idle":"2025-11-07T09:46:40.707306Z","shell.execute_reply.started":"2025-11-07T09:46:40.666700Z","shell.execute_reply":"2025-11-07T09:46:40.706642Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"‚úÖ MemoryEfficientRAGPipeline loaded (6-7GB RAM instead of 15GB)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ======================== CELL 4: INITIALIZE MEMORY-EFFICIENT PIPELINE ==========================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üöÄ INITIALIZING MEMORY-EFFICIENT PIPELINE\")\nprint(\"=\"*80 + \"\\n\")\n\npipeline = MemoryEfficientRAGPipeline(config, DOMAINS)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ PIPELINE READY!\")\nprint(\"üíæ Startup RAM: ~2GB (Peak during query: 6-7GB)\")\nprint(\"=\"*80)\n","metadata":{"_uuid":"ca863b05-5ab6-4750-ae2a-6b3c50d642ce","_cell_guid":"e1782af3-9bd1-465c-800b-9e7f9bbdab10","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T09:46:44.914359Z","iopub.execute_input":"2025-11-07T09:46:44.914935Z","iopub.status.idle":"2025-11-07T09:46:49.197239Z","shell.execute_reply.started":"2025-11-07T09:46:44.914913Z","shell.execute_reply":"2025-11-07T09:46:49.196506Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüöÄ INITIALIZING MEMORY-EFFICIENT PIPELINE\n================================================================================\n\n================================================================================\nüè• INITIALIZING MEDICAL RAG SYSTEM\n================================================================================\n\nüì¶ Loading lightweight embedder...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8afdc0cd46044af2a4ad6e3c51496e0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681b8b5f3edc4fac95c2b665c40486a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c948ba44589d4e7b80dc1a4059625500"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"712b93dbc108442cb3790f908a4a7977"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3fc03955b049428ef4c97806d52d75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccc0c93b45c4bf78fa218d5163a9cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"765680c6a41241ab86f518151414e39f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5040aab6674a648815fe5a60d1f8f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3866ac699b554a93b6f4fb72a84e4c1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe7e8c431ea24592bbbf6e35cddcbba6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac7b580ee614b3b8ae58fc63b777335"}},"metadata":{}},{"name":"stdout","text":"  ‚úÖ Embedder loaded (80MB)\n\n‚úÖ Pipeline initialized\nüíæ Domains: 11 (will load on-demand)\n================================================================================\n\n================================================================================\n‚úÖ PIPELINE READY!\nüíæ Startup RAM: ~2GB (Peak during query: 6-7GB)\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ======================== CELL 5: INTERACTIVE MODE ==========================\n\ndef ask_question():\n    \"\"\"Interactive medical Q&A\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üí¨ INTERACTIVE MEDICAL QA MODE\")\n    print(\"=\"*80)\n    print(\"Type your medical questions below.\")\n    print(\"Type 'quit' or 'exit' to stop.\\n\")\n    \n    while True:\n        query = input(\"\\nüîç Your Question: \").strip()\n        \n        if not query:\n            print(\"‚ö†Ô∏è  Please enter a question\")\n            continue\n        \n        if query.lower() in ['quit', 'exit', 'stop', 'q']:\n            print(\"\\nüëã Goodbye!\")\n            break\n        \n        print(\"\\n\" + \"-\"*80)\n        \n        try:\n            result = pipeline.run_query(query)\n            \n            print(f\"\\nüí° **ANSWER:**\")\n            print(f\"{result['answer']}\\n\")\n            \n            print(f\"üìä Confidence: {result['metrics']['composite']:.2f}\")\n            print(f\"üéØ Knowledge Domains: {', '.join(result['domains'])}\")\n            print(f\"‚è±Ô∏è  Response Time: {result['processing_time']:.2f}s\")\n            \n            if result['sources']:\n                show_sources = input(\"\\nüìö Show sources? (y/n): \").strip().lower()\n                if show_sources == 'y':\n                    print(\"\\nTop Sources:\")\n                    for i, source in enumerate(result['sources'][:3], 1):\n                        print(f\"\\n{i}. [{source['domain']}] Relevance: {source['score']:.2f}\")\n                        print(f\"   {source['chunk']}\")\n        \n        except Exception as e:\n            print(f\"\\n‚ùå Error: {e}\")\n        \n        print(\"\\n\" + \"-\"*80)\n\n# Run\nask_question()\n","metadata":{"_uuid":"c009c4b1-112f-47f6-9a9e-49c45cdc7d7f","_cell_guid":"f580cf23-fb1e-4c6e-aa0b-016ab6b69cce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T09:46:51.529582Z","iopub.execute_input":"2025-11-07T09:46:51.530355Z","iopub.status.idle":"2025-11-07T10:04:55.380530Z","shell.execute_reply.started":"2025-11-07T09:46:51.530330Z","shell.execute_reply":"2025-11-07T10:04:55.379595Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüí¨ INTERACTIVE MEDICAL QA MODE\n================================================================================\nType your medical questions below.\nType 'quit' or 'exit' to stop.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nüîç Your Question:  I'm 35, on metformin for diabetes and sertraline for anxiety. Having irregular periods, mood swings, weight gain. Could medications cause this?\n"},{"name":"stdout","text":"\n--------------------------------------------------------------------------------\n\nüîç Query: I'm 35, on metformin for diabetes and sertraline for anxiety. Having irregular periods, mood swings, weight gain. Could medications cause this?\n  üìÇ Loading general_medical index...\n    ‚úÖ Loaded 710919 chunks\n  üìÇ Loading mental_health index...\n    ‚úÖ Loaded 22565 chunks\n  üìÇ Loading ophthalmology index...\n    ‚úÖ Loaded 57979 chunks\n  üìÇ Loading pediatrics index...\n    ‚úÖ Loaded 19888 chunks\n  üìÇ Loading symptoms_triage index...\n    ‚úÖ Loaded 147907 chunks\nüìç Domains: general_medical\nüîé Retrieving information...\nüéØ Reranking...\n  üì¶ Loading reranker...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a4fd8c423c740f99d2016f9c5cba0d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c7b28c0e8ba463e88532cf7307bdd70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3af5ada8bc68416aa76ae95a72ba088d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b94fdc58e044bc492af50ad6e013887"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1084399c49f54a2f91384c8150fce2ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0376d099d41b49afabab19721bc438ea"}},"metadata":{}},{"name":"stdout","text":"    ‚úÖ Reranker loaded (300MB)\n  üóëÔ∏è  Reranker unloaded\nüí¨ Generating answer...\n  üì¶ Loading generator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8f6573f3b0244a79893ae2c78f9d259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07ecc3a83244b939043c0fa7634178f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0920924cc01d49aa8cec0350158af20c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b548e234fe49519518a917740941d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de340d5211ba42a78ca61c088feb601e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6301debc3bca4df191f05263f65999f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a627064621634090bb822836a9b58b04"}},"metadata":{}},{"name":"stdout","text":"    ‚úÖ Generator loaded (900MB)\n  üóëÔ∏è  Generator unloaded\n  üóëÔ∏è  Freed memory from 4 domains\n‚úÖ Done in 310.97s (confidence: 0.64)\n\nüí° **ANSWER:**\nHowever, it is possible that some persons may experience significant mood or anxiety symptoms due to the medication. If your anxiety symptoms are not severe, the would suggest that you try relaxation exercises like deep breathing, progressive muscle relaxation, yoga, meditation, etc. and try to stay as stress-free as possible.\n\n‚ö†Ô∏è Please consult a healthcare professional for personalized medical advice.\n\nüìä Confidence: 0.64\nüéØ Knowledge Domains: general_medical\n‚è±Ô∏è  Response Time: 310.97s\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nüìö Show sources? (y/n):  y\n"},{"name":"stdout","text":"\nTop Sources:\n\n1. [general_medical] Relevance: 0.79\n   However, it is possible that some persons may experience significant mood or anxiety symptoms due to the medication. If your anxiety symptoms are not \n\n2. [general_medical] Relevance: 0.68\n   Anti depressants such as sertraline can provide relief with your complaints of low mood and reduced energy levels, etc. Discuss with the treating onco\n\n3. [general_medical] Relevance: 0.66\n   Metformin helps in treating the insulin resistance. The possibility of pregnancy depends on whether ovulation is occurring or not. As you are having i\n\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nüîç Your Question:  I have sudden severe chest pain radiating to left arm, shortness of breath, sweating heavily. I'm on metoprolol. What should I do?\n"},{"name":"stdout","text":"\n--------------------------------------------------------------------------------\n\nüîç Query: I have sudden severe chest pain radiating to left arm, shortness of breath, sweating heavily. I'm on metoprolol. What should I do?\nüö® EMERGENCY DETECTED\nüìç Domains: symptoms_triage\nüí¨ Generating answer...\n  üóëÔ∏è  Freed memory from 1 domains\n‚úÖ Done in 1.10s (confidence: 0.95)\n\nüí° **ANSWER:**\nüö® **EMERGENCY - SEEK IMMEDIATE MEDICAL ATTENTION**\n\nPlease call 911 or go to the nearest emergency room immediately. Based on your symptoms, you may have a life-threatening condition.\n\n‚ö†Ô∏è This is an emergency. Do not delay.\n\nüìä Confidence: 0.95\nüéØ Knowledge Domains: symptoms_triage\n‚è±Ô∏è  Response Time: 1.10s\n\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nüîç Your Question:  My 82-year-old grandmother on metformin developed sudden cognitive decline. Doctor says it's normal aging, but she was sharp 3 weeks ago. Is metformin causing this?\n"},{"name":"stdout","text":"\n--------------------------------------------------------------------------------\n\nüîç Query: My 82-year-old grandmother on metformin developed sudden cognitive decline. Doctor says it's normal aging, but she was sharp 3 weeks ago. Is metformin causing this?\n  üìÇ Loading general_medical index...\n    ‚úÖ Loaded 710919 chunks\n  üìÇ Loading mental_health index...\n    ‚úÖ Loaded 22565 chunks\n  üìÇ Loading ophthalmology index...\n    ‚úÖ Loaded 57979 chunks\n  üìÇ Loading pediatrics index...\n    ‚úÖ Loaded 19888 chunks\n  üìÇ Loading symptoms_triage index...\n    ‚úÖ Loaded 147907 chunks\nüìç Domains: general_medical\nüîé Retrieving information...\nüéØ Reranking...\n  üì¶ Loading reranker...\n    ‚úÖ Reranker loaded (300MB)\n  üóëÔ∏è  Reranker unloaded\nüí¨ Generating answer...\n  üóëÔ∏è  Freed memory from 4 domains\n‚úÖ Done in 288.55s (confidence: 0.62)\n\nüí° **ANSWER:**\nShe is 83 years old, and it is common to see cognitive decline at this age. The problems of memory loss, inability to remember numbers or passwords could occur due to age related cognitive decline. But she has also shown symptoms like \\\"a blue shroud passing from one side of her head to other\\\" and this could occur due to some psychotic symptoms.\n\n‚ö†Ô∏è Consult a healthcare professional.\n\nüìä Confidence: 0.62\nüéØ Knowledge Domains: general_medical\n‚è±Ô∏è  Response Time: 288.55s\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nüìö Show sources? (y/n):  y\n"},{"name":"stdout","text":"\nTop Sources:\n\n1. [general_medical] Relevance: 0.69\n   She is 83 years old, and it is common to see cognitive decline at this age. The problems of memory loss, inability to remember numbers or passwords co\n\n2. [general_medical] Relevance: 0.52\n   can help. These medicines will reduce the progression of her cognitive decline, and she will feel better. Provide her warm loving support, and she wil\n\n3. [general_medical] Relevance: 0.49\n   These medicines will reduce the progression of her cognitive decline, and she will feel better. Provide her warm loving support, and she will show imp\n\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nüîç Your Question:  exit\n"},{"name":"stdout","text":"\nüëã Goodbye!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}