{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13573706,"sourceType":"datasetVersion","datasetId":8622866}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================== CELL 1: FIX DEPENDENCIES ==========================\nimport subprocess\nimport sys\n\nprint('üîß Fixing all package dependencies and kernel state...')\n\n# STEP 1: Wipe any incompatible pyarrow\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"], capture_output=True, check=False)\n\n# STEP 2: Install pyarrow and essentials ONLY once before next cell\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyarrow==15.0.2\", \"keybert\", \"rank-bm25\", \"evaluate\", \"faiss-cpu\", \"protobuf<5.0.0\", \"sacremoses\"], check=True)\n\n# STEP 3: Install bigframes dependencies if required by your notebook/business use\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rich==13.7.1\"], check=True)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"google-cloud-bigquery-storage>=2.30.0\"], check=True)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"google-cloud-bigquery>=3.31.0\"], check=True)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"google-api-core>=2.10.2\"], check=True)\n\nprint('‚úÖ Dependencies fixed and installed. Now RESTART the kernel and run Cell 2.')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================== CELL 2: SANITY CHECK ALL IMPORTS =====================\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"üîç Verifying critical imports...\")\n\ntry:\n    from datasets import load_dataset\n    print(\"‚úÖ datasets\")\n    from sentence_transformers import SentenceTransformer\n    print(\"‚úÖ sentence-transformers\")\n    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n    print(\"‚úÖ transformers\")\n    import faiss\n    print(\"‚úÖ faiss\")\n    from keybert import KeyBERT\n    print(\"‚úÖ keybert\")\n    from rank_bm25 import BM25Okapi\n    print(\"‚úÖ rank-bm25\")\n    import torch\n    print(f\"‚úÖ torch (device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n    print(\"\\nüéâ ALL CRITICAL IMPORTS SUCCESSFUL - You can now run your full pipeline!\")\nexcept Exception as e:\n    print(f\"‚ùå Import failed: {e}\\nPlease RESTART THE KERNEL and run Cell 1 again.\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T05:18:08.576935Z","iopub.execute_input":"2025-11-01T05:18:08.577392Z","iopub.status.idle":"2025-11-01T05:18:37.228543Z","shell.execute_reply.started":"2025-11-01T05:18:08.577369Z","shell.execute_reply":"2025-11-01T05:18:37.227899Z"}},"outputs":[{"name":"stdout","text":"üîç Verifying critical imports...\n‚úÖ datasets\n","output_type":"stream"},{"name":"stderr","text":"2025-11-01 05:18:21.039471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761974301.207939     128 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761974301.253740     128 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"‚úÖ sentence-transformers\n‚úÖ transformers\n‚úÖ faiss\n‚úÖ keybert\n‚úÖ rank-bm25\n‚úÖ torch (device: cuda)\n\nüéâ ALL CRITICAL IMPORTS SUCCESSFUL - You can now run your full pipeline!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============== CELL 3: PRODUCTION-READY MULTI-DOMAIN RAG PIPELINE ==============\n\nimport re\nimport json\nimport time\nimport pickle\nimport random\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Any\n\nimport numpy as np\nimport torch\nimport faiss\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    AutoModelForSequenceClassification\n)\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom rank_bm25 import BM25Okapi\nfrom keybert import KeyBERT\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"üîß Using device: {device}\")\n\n# ============================================================================\n# CONFIGURATION - PRODUCTION OPTIMIZED\n# ============================================================================\n\n@dataclass\nclass DomainConfig:\n    name: str\n    dataset_name: str\n    config_name: str = None\n    dataset_split: str = \"train\"\n    index_path: str = None\n    id2doc_path: str = None\n    metadata_path: str = None\n    \n    def __post_init__(self):\n        if self.index_path is None:\n            self.index_path = f\"{self.name}_faiss.index\"\n        if self.id2doc_path is None:\n            self.id2doc_path = f\"{self.name}_id2doc.pkl\"\n        if self.metadata_path is None:\n            self.metadata_path = f\"{self.name}_metadata.json\"\n\n@dataclass\nclass RAGConfig:\n    embed_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    reranker_model: str = \"BAAI/bge-reranker-large\"\n    hyde_model: str = \"microsoft/BioGPT-Large\"\n    generator_model: str = \"microsoft/BioGPT-Large\"\n    chunk_window: int = 3\n    chunk_stride: int = 1\n    retrieve_k: int = 30\n    rerank_topk: int = 8\n    context_chunks: int = 6  # INCREASED from 4\n    hyde_weight: float = 0.4\n    faiss_alpha: float = 0.6\n    max_new_tokens: int = 400  # INCREASED from 200\n    hyde_max_tokens: int = 80  # INCREASED from 60\n    completeness_threshold: float = 0.65\n    faithfulness_threshold: float = 0.55\n    retrieval_weight: float = 0.4\n    completeness_weight: float = 0.3\n    faithfulness_weight: float = 0.3\n    prompts_log: str = \"prompts_outputs.pkl\"\n    random_seed: int = 42\n    test_size: float = 0.15\n\nDOMAINS = [\n    DomainConfig(\n        name=\"women_health\",\n        dataset_name=\"altaidevorg/women-health-mini\",\n        index_path=\"/kaggle/input/indexes/women_health_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexes/women_health_id2doc.pkl\",\n        metadata_path=\"/kaggle/input/indexes/women_health_metadata.json\"\n    ),\n    DomainConfig(\n        name=\"medical_qa\",\n        dataset_name=\"Malikeh1375/medical-question-answering-datasets\",\n        config_name=\"all-processed\",\n        index_path=\"/kaggle/input/indexes/medical_qa_faiss.index\",\n        id2doc_path=\"/kaggle/input/indexes/medical_qa_id2doc.pkl\",\n        metadata_path=\"/kaggle/input/indexes/medical_qa_metadata.json\"\n    )\n]\n\nconfig = RAGConfig()\n\n# ============================================================================\n# UTILITY FUNCTIONS\n# ============================================================================\n\ndef clean_text_artifacts(text: str) -> str:\n    text = re.sub(r\"^(Answer:|Final answer:|Response:|Detailed Medical Answer:)\\s*\", \"\", text, flags=re.IGNORECASE)\n    text = re.sub(r\"<\\/?[^>]+>|</s>|‚ñÉ|\\[INST\\]|\\[/INST\\]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text.strip(\" \\n\\r\\t\\\"'\")\n\ndef monitor_memory():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        logger.info(f\"üíæ GPU: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n        if allocated/total > 0.85:\n            torch.cuda.empty_cache()\n\n# ============================================================================\n# DATASET LOADING\n# ============================================================================\n\nclass DatasetLoader:\n    @staticmethod\n    def extract_qa_pairs(dataset, domain_name: str) -> list:\n        qa_data = []\n        for idx, row in enumerate(dataset):\n            try:\n                if 'conversations' in row and isinstance(row['conversations'], list):\n                    conversations = row['conversations']\n                    question, answer = \"\", \"\"\n                    for msg in conversations:\n                        if msg.get(\"role\") == \"user\" and not question:\n                            question = msg.get(\"content\", \"\")\n                        if msg.get(\"role\") == \"assistant\" and not answer:\n                            answer = msg.get(\"content\", \"\")\n                    if question and answer:\n                        qa_data.append({\n                            \"question\": question.strip(),\n                            \"answer\": answer.strip(),\n                            \"domain\": domain_name,\n                            \"source_id\": idx\n                        })\n                        continue\n                if 'question' in row and 'answer' in row:\n                    qa_data.append({\n                        \"question\": str(row['question']).strip(),\n                        \"answer\": str(row['answer']).strip(),\n                        \"domain\": domain_name,\n                        \"source_id\": idx\n                    })\n                    continue\n                if 'input' in row and 'output' in row:\n                    qa_data.append({\n                        \"question\": str(row['input']).strip(),\n                        \"answer\": str(row['output']).strip(),\n                        \"domain\": domain_name,\n                        \"source_id\": idx\n                    })\n            except Exception as e:\n                if idx < 2:\n                    print(f\"extract_qa_pairs WARNING, row {idx}: {e}\")\n                continue\n        return qa_data\n    \n    @staticmethod\n    def load_domain_data(domain_config: DomainConfig) -> tuple:\n        logger.info(f\"üì• Loading {domain_config.name}...\")\n        try:\n            if domain_config.config_name:\n                dataset = load_dataset(domain_config.dataset_name, domain_config.config_name, split=domain_config.dataset_split)\n            else:\n                dataset = load_dataset(domain_config.dataset_name, split=domain_config.dataset_split)\n            logger.info(f\"Dataset {domain_config.name} loaded with {len(dataset)} rows\")\n            qa_data = DatasetLoader.extract_qa_pairs(dataset, domain_config.name)\n            if not qa_data:\n                logger.error(f\"No QA pairs extracted from {domain_config.name}\")\n                logger.error(f\"Sample row structure: {dataset[0]}\")\n                raise ValueError(f\"No QA pairs extracted from {domain_config.name}. Check dataset structure.\")\n            train_data, test_data = train_test_split(\n                qa_data, test_size=config.test_size, random_state=config.random_seed\n            )\n            logger.info(f\"‚úÖ {domain_config.name}: {len(train_data)} train, {len(test_data)} test\")\n            return train_data, test_data\n        except Exception as e:\n            logger.error(f\"‚ùå Failed to load {domain_config.name}: {e}\")\n            raise\n\n# ============================================================================\n# TEXT CHUNKING\n# ============================================================================\n\nclass TextChunker:\n    @staticmethod\n    def create_chunks(data: List[Dict], window: int = 3, stride: int = 1, min_chars: int = 50) -> List[Dict]:\n        chunks = []\n        for item in data:\n            text = item.get(\"answer\", \"\")\n            if not text or len(text) < min_chars:\n                continue\n            \n            sentences = sent_tokenize(text)\n            if not sentences:\n                continue\n            \n            if len(sentences) <= window:\n                chunks.append({\n                    \"chunk\": \" \".join(sentences),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks)\n                })\n                continue\n            \n            for i in range(0, max(1, len(sentences) - window + 1), stride):\n                chunks.append({\n                    \"chunk\": \" \".join(sentences[i:i + window]),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks),\n                    \"window\": (i, i + window)\n                })\n        return chunks\n\n# ============================================================================\n# MODEL MANAGEMENT\n# ============================================================================\n\nclass ModelManager:\n    def __init__(self, config: RAGConfig, device: torch.device):\n        self.config = config\n        self.device = device\n        self.models = {}\n    \n    def load_embedder(self):\n        logger.info(f\"üì¶ Loading embedder...\")\n        embedder = SentenceTransformer(self.config.embed_model, device=self.device)\n        self.models['embedder'] = embedder\n        logger.info(f\"‚úÖ Embedder loaded\")\n        return embedder\n    \n    def load_reranker(self):\n        logger.info(f\"üì¶ Loading reranker...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.reranker_model)\n        model = AutoModelForSequenceClassification.from_pretrained(\n            self.config.reranker_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        ).to(self.device)\n        model.eval()\n        self.models['reranker_tokenizer'] = tokenizer\n        self.models['reranker_model'] = model\n        logger.info(f\"‚úÖ Reranker loaded\")\n        return tokenizer, model\n    \n    def load_hyde_model(self):\n        logger.info(f\"üì¶ Loading HyDE model...\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(self.config.hyde_model)\n            model = AutoModelForCausalLM.from_pretrained(\n                self.config.hyde_model,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                low_cpu_mem_usage=True\n            ).to(self.device)\n            model.eval()\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            self.models['hyde_tokenizer'] = tokenizer\n            self.models['hyde_model'] = model\n            logger.info(f\"‚úÖ HyDE model loaded\")\n            return tokenizer, model\n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è HyDE load failed, using query expansion: {e}\")\n            return None, None\n    \n    def load_generator(self):\n        logger.info(f\"üì¶ Loading generator...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.generator_model)\n        model = AutoModelForCausalLM.from_pretrained(\n            self.config.generator_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            low_cpu_mem_usage=True\n        ).to(self.device)\n        model.eval()\n        self.models['gen_tokenizer'] = tokenizer\n        self.models['gen_model'] = model\n        logger.info(f\"‚úÖ Generator loaded\")\n        return tokenizer, model\n    \n    def load_keyword_extractor(self):\n        try:\n            kw_model = KeyBERT(model=self.models.get('embedder'))\n            self.models['keyword_extractor'] = kw_model\n            logger.info(f\"‚úÖ KeyBERT loaded\")\n            return kw_model\n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è KeyBERT load failed: {e}\")\n            return None\n    \n    def load_all(self):\n        logger.info(\"üîß Loading all models...\")\n        self.load_embedder()\n        self.load_reranker()\n        self.load_hyde_model()\n        self.load_generator()\n        self.load_keyword_extractor()\n        monitor_memory()\n        logger.info(\"‚úÖ All models loaded\")\n        return self.models\n\n# ============================================================================\n# INDEX MANAGEMENT\n# ============================================================================\n\nclass MultiDomainIndexManager:\n    def __init__(self, config: RAGConfig, embedder: SentenceTransformer):\n        self.config = config\n        self.embedder = embedder\n        self.domain_indices = {}\n    \n    def build_or_load_domain_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        if Path(domain_config.index_path).exists() and Path(domain_config.id2doc_path).exists():\n            try:\n                return self._load_existing_index(domain_config)\n            except:\n                pass\n        return self._build_new_index(domain_config, chunks)\n    \n    def _load_existing_index(self, domain_config: DomainConfig) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"üìÇ Loading existing {domain_config.name} index...\")\n        index = faiss.read_index(domain_config.index_path)\n        with open(domain_config.id2doc_path, \"rb\") as f:\n            id2doc = pickle.load(f)\n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        logger.info(f\"‚úÖ Loaded {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def _build_new_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"üî® Building {domain_config.name} index...\")\n        id2doc = [chunk[\"chunk\"] for chunk in chunks]\n        \n        embeddings = self.embedder.encode(\n            id2doc, normalize_embeddings=True, show_progress_bar=True,\n            batch_size=64, convert_to_numpy=True\n        ).astype('float32')\n        \n        dim = embeddings.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(embeddings)\n        \n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        \n        faiss.write_index(index, domain_config.index_path)\n        with open(domain_config.id2doc_path, \"wb\") as f:\n            pickle.dump(id2doc, f)\n        \n        metadata = {\"created_at\": time.time(), \"n_vectors\": int(index.ntotal), \"embedding_dim\": dim, \"domain\": domain_config.name}\n        with open(domain_config.metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n        \n        logger.info(f\"‚úÖ Built {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def load_all_domains(self, domain_chunks: Dict[str, List[Dict]]):\n        for domain in DOMAINS:\n            index, id2doc, bm25 = self.build_or_load_domain_index(domain, domain_chunks.get(domain.name, []))\n            self.domain_indices[domain.name] = {\n                'index': index, 'id2doc': id2doc, 'bm25': bm25, 'config': domain\n            }\n        logger.info(f\"‚úÖ Loaded {len(self.domain_indices)} domain indices\")\n\n# ============================================================================\n# QUERY ROUTER\n# ============================================================================\n\nclass QueryRouter:\n    def __init__(self, embedder: SentenceTransformer, domain_indices: Dict):\n        self.embedder = embedder\n        self.domain_indices = domain_indices\n        self.domain_centroids = self._compute_centroids()\n    \n    def _compute_centroids(self) -> Dict[str, np.ndarray]:\n        centroids = {}\n        logger.info(\"üéØ Computing domain centroids...\")\n        for domain_name, domain_data in self.domain_indices.items():\n            id2doc = domain_data['id2doc']\n            sample_docs = random.sample(id2doc, min(500, len(id2doc)))\n            embeddings = self.embedder.encode(sample_docs, normalize_embeddings=True, convert_to_numpy=True)\n            centroids[domain_name] = embeddings.mean(axis=0)\n        return centroids\n    \n    def route_query(self, query: str, top_k: int = 2) -> List[str]:\n        query_emb = self.embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n        similarities = {domain: float(np.dot(query_emb, centroid)) for domain, centroid in self.domain_centroids.items()}\n        sorted_domains = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        selected = [d[0] for d in sorted_domains[:top_k]]\n        logger.info(f\"üß≠ Routed to: {selected}\")\n        return selected\n\n# ============================================================================\n# MULTI-DOMAIN RAG PIPELINE - PRODUCTION VERSION\n# ============================================================================\n\nclass MultiDomainRAGPipeline:\n    def __init__(self, config: RAGConfig, domains: List[DomainConfig]):\n        self.config = config\n        self.domains = domains\n        self.device = device\n        \n        self.model_manager = ModelManager(config, device)\n        self.models = self.model_manager.load_all()\n        \n        self.data = {}\n        self.test_data = {}\n        domain_chunks = {}\n        \n        for domain in domains:\n            train_data, test_data = DatasetLoader.load_domain_data(domain)\n            self.data[domain.name] = train_data\n            self.test_data[domain.name] = test_data\n            chunks = TextChunker.create_chunks(train_data, window=config.chunk_window, stride=config.chunk_stride)\n            domain_chunks[domain.name] = chunks\n        \n        self.index_manager = MultiDomainIndexManager(config, self.models['embedder'])\n        self.index_manager.load_all_domains(domain_chunks)\n        \n        self.router = QueryRouter(self.models['embedder'], self.index_manager.domain_indices)\n        self.prompts_log = []\n        \n        logger.info(\"‚úÖ Multi-domain RAG pipeline initialized\")\n    \n    def is_medical_query(self, query: str) -> bool:\n        \"\"\"Check if query is medical/health-related. NEW METHOD\"\"\"\n        medical_keywords = [\n            'symptom', 'disease', 'treatment', 'diagnosis', 'health', 'medical',\n            'doctor', 'patient', 'pain', 'fever', 'infection', 'condition',\n            'medicine', 'drug', 'therapy', 'screening', 'test', 'exam',\n            'pregnancy', 'diabetes', 'cancer', 'blood', 'pressure', 'heart',\n            'period', 'menstrual', 'hormone', 'pregnant', 'ovary', 'breast'\n        ]\n        query_lower = query.lower()\n        return any(keyword in query_lower for keyword in medical_keywords)\n    \n    def generate_hyde_answer(self, query: str) -> str:\n        if self.models['hyde_model'] is None:\n            return query\n        \n        # IMPROVED PROMPT\n        prompt = f\"Medical Question: {query}\\nDetailed Medical Answer with relevant terminology:\"\n        try:\n            inputs = self.models['hyde_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['hyde_model'].generate(\n                    **inputs, max_new_tokens=self.config.hyde_max_tokens,\n                    do_sample=False, pad_token_id=self.models['hyde_tokenizer'].eos_token_id,\n                    repetition_penalty=1.15\n                )\n            text = self.models['hyde_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            hyde = clean_text_artifacts(text.split(\"Answer:\")[-1])\n            return hyde if hyde else query\n        except:\n            return query\n    \n    def retrieve_from_domain(self, query: str, domain_name: str, k: int) -> List[Tuple[int, float, str]]:\n        domain_data = self.index_manager.domain_indices[domain_name]\n        index = domain_data['index']\n        id2doc = domain_data['id2doc']\n        bm25 = domain_data['bm25']\n        \n        hyde_text = self.generate_hyde_answer(query)\n        q_emb = self.models['embedder'].encode([query], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        h_emb = self.models['embedder'].encode([hyde_text], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        merged_emb = (1 - self.config.hyde_weight) * q_emb + self.config.hyde_weight * h_emb\n        \n        D, I = index.search(merged_emb, k)\n        faiss_scores = D[0]\n        if faiss_scores.max() > faiss_scores.min():\n            faiss_norm = (faiss_scores - faiss_scores.min()) / (faiss_scores.max() - faiss_scores.min())\n        else:\n            faiss_norm = np.ones_like(faiss_scores)\n        faiss_map = {int(idx): float(score) for idx, score in zip(I[0], faiss_norm)}\n        \n        bm25_scores = bm25.get_scores(word_tokenize(query.lower()))\n        if bm25_scores.max() > bm25_scores.min():\n            bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())\n        else:\n            bm25_norm = np.zeros_like(bm25_scores)\n        \n        candidates = set(I[0].tolist()) | set(np.argsort(bm25_scores)[::-1][:k].tolist())\n        merged_scores = []\n        for idx in candidates:\n            f = faiss_map.get(int(idx), 0.0)\n            b = float(bm25_norm[int(idx)]) if int(idx) < len(bm25_norm) else 0.0\n            score = self.config.faiss_alpha * f + (1 - self.config.faiss_alpha) * b\n            merged_scores.append((int(idx), score, domain_name))\n        \n        merged_scores.sort(key=lambda x: x[1], reverse=True)\n        return merged_scores[:k]\n    \n    def rerank_candidates(self, query: str, candidates: List[Tuple[int, float, str]]) -> List[Tuple[str, float, str]]:\n        texts, metadata = [], []\n        for idx, score, domain_name in candidates:\n            domain_data = self.index_manager.domain_indices[domain_name]\n            text = domain_data['id2doc'][idx]\n            texts.append(text)\n            metadata.append((idx, domain_name))\n        \n        reranker_scores = []\n        batch_size = 8\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = self.models['reranker_tokenizer'](\n                [query] * len(batch_texts), batch_texts,\n                padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n            ).to(self.device)\n            \n            with torch.no_grad():\n                outputs = self.models['reranker_model'](**inputs)\n                logits = outputs.logits.cpu().numpy()\n            \n            for lg in logits:\n                if lg.shape == ():\n                    score = float(lg)\n                elif len(lg.shape) == 1 and lg.shape[0] == 1:\n                    score = float(lg[0])\n                elif len(lg.shape) == 1 and lg.shape[0] == 2:\n                    score = float(lg[1])\n                else:\n                    score = float(np.max(lg))\n                reranker_scores.append(score)\n        \n        reranked = [(texts[i], reranker_scores[i], metadata[i][1]) for i in range(len(texts))]\n        reranked.sort(key=lambda x: x[1], reverse=True)\n        return reranked[:self.config.rerank_topk]\n    \n    def generate_answer(self, query: str, contexts: List[Tuple[str, float, str]]) -> str:\n        context_parts = [f\"[Source {i+1} from {domain}]:\\n{text}\" \n                        for i, (text, score, domain) in enumerate(contexts[:self.config.context_chunks])]\n        context_block = \"\\n\\n\".join(context_parts)\n        \n        # IMPROVED PROMPT\n        prompt = f\"\"\"You are a medical expert assistant. Using the following retrieved medical information, provide a comprehensive, accurate, and helpful answer to the patient's question.\n\nIf the question asks about:\n- A condition: Identify it clearly and describe symptoms\n- Tests: List specific tests and what they measure  \n- Treatment: Describe available treatment options with details\n- Multiple parts: Address each part of the question systematically\n\nAlways base your answer strictly on the provided sources. If information is insufficient, acknowledge that and suggest consulting a healthcare provider.\n\nMedical Context:\n{context_block}\n\nPatient Question: {query}\n\nDetailed Medical Answer:\"\"\"\n        \n        try:\n            inputs = self.models['gen_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n            with torch.no_grad():\n                # IMPROVED GENERATION PARAMETERS\n                outputs = self.models['gen_model'].generate(\n                    **inputs, \n                    max_new_tokens=self.config.max_new_tokens,\n                    min_new_tokens=50,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    pad_token_id=self.models['gen_tokenizer'].eos_token_id,\n                    repetition_penalty=1.2,\n                    no_repeat_ngram_size=3\n                )\n            raw = self.models['gen_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            answer = clean_text_artifacts(raw.split(\"Detailed Medical Answer:\")[-1])\n            \n            # POST-PROCESSING\n            sentences = sent_tokenize(answer)\n            if sentences and len(sentences[-1]) < 20:\n                answer = \" \".join(sentences[:-1])\n            \n            if len(answer.split()) < 10:\n                answer = \"I apologize, but I don't have enough specific information in my knowledge base to provide a complete answer to your question. Please consult with a healthcare professional for personalized medical advice.\"\n            \n            self.prompts_log.append({\n                \"type\": \"generate\", \"query\": query,\n                \"contexts\": [(t, d) for t, _, d in contexts[:self.config.context_chunks]],\n                \"prompt\": prompt, \"raw\": raw, \"answer\": answer, \"timestamp\": time.time()\n            })\n            \n            return answer if answer else \"Insufficient information. Please consult a healthcare professional.\"\n        except Exception as e:\n            logger.error(f\"Generation failed: {e}\")\n            return \"Error generating answer. Please consult a healthcare professional.\"\n    \n    def compute_metrics(self, query: str, answer: str, contexts: List[Tuple[str, float, str]]) -> Dict[str, float]:\n        metrics = {}\n        \n        # FIXED RETRIEVAL NORMALIZATION\n        if contexts:\n            raw_scores = [score for _, score, _ in contexts[:self.config.context_chunks]]\n            retrieval_score = np.mean(raw_scores)\n            metrics['retrieval'] = float(np.clip((retrieval_score + 10) / 20, 0, 1))\n        else:\n            metrics['retrieval'] = 0.0\n        \n        try:\n            context_texts = [text for text, _, _ in contexts[:self.config.context_chunks]]\n            all_keywords = []\n            \n            if self.models['keyword_extractor']:\n                for ctx_text in context_texts:\n                    keywords = self.models['keyword_extractor'].extract_keywords(\n                        ctx_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5\n                    )\n                    all_keywords.extend([kw for kw, _ in keywords])\n            \n            unique_keywords = list(dict.fromkeys([kw.lower() for kw in all_keywords if kw]))\n            \n            if unique_keywords and answer:\n                answer_emb = self.models['embedder'].encode([answer], normalize_embeddings=True, convert_to_tensor=True)\n                keyword_embs = self.models['embedder'].encode(unique_keywords, normalize_embeddings=True, convert_to_tensor=True)\n                similarities = util.cos_sim(answer_emb, keyword_embs).cpu().numpy()[0]\n                covered = (similarities >= self.config.completeness_threshold).sum()\n                metrics['completeness'] = float(covered / len(unique_keywords))\n            else:\n                metrics['completeness'] = 0.0\n        except:\n            metrics['completeness'] = 0.0\n        \n        try:\n            if answer and contexts:\n                answer_sentences = sent_tokenize(answer)\n                context_sentences = []\n                for text, _, _ in contexts[:self.config.context_chunks]:\n                    context_sentences.extend(sent_tokenize(text))\n                \n                if answer_sentences and context_sentences:\n                    ans_embs = self.models['embedder'].encode(answer_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    ctx_embs = self.models['embedder'].encode(context_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    sim_matrix = util.cos_sim(ans_embs, ctx_embs).cpu().numpy()\n                    max_sims = np.max(sim_matrix, axis=1)\n                    faithful = (max_sims >= self.config.faithfulness_threshold).sum()\n                    metrics['faithfulness'] = float(faithful / len(answer_sentences))\n                else:\n                    metrics['faithfulness'] = 0.0\n            else:\n                metrics['faithfulness'] = 0.0\n        except:\n            metrics['faithfulness'] = 0.0\n        \n        metrics['composite'] = (\n            self.config.retrieval_weight * metrics['retrieval'] +\n            self.config.completeness_weight * metrics['completeness'] +\n            self.config.faithfulness_weight * metrics['faithfulness']\n        )\n        \n        return metrics\n    \n    def run_query(self, query: str, top_domains: int = 2, log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"üîç Processing: {query[:100]}...\")\n        \n        # OUT-OF-DOMAIN CHECK\n        if not self.is_medical_query(query):\n            return {\n                \"query\": query,\n                \"routed_domains\": [],\n                \"answer\": \"I apologize, but I'm designed to answer medical and health-related questions. Your question doesn't appear to be medical in nature. Could you please ask a health or medical question?\",\n                \"contexts\": [],\n                \"metrics\": {\"retrieval\": 0.0, \"completeness\": 0.0, \"faithfulness\": 0.0, \"composite\": 0.0}\n            }\n        \n        selected_domains = self.router.route_query(query, top_k=top_domains)\n        \n        all_candidates = []\n        for domain_name in selected_domains:\n            candidates = self.retrieve_from_domain(query, domain_name, k=self.config.retrieve_k)\n            all_candidates.extend(candidates)\n        \n        if log_diagnostics:\n            logger.info(f\"Retrieved {len(all_candidates)} candidates from {len(selected_domains)} domains\")\n        \n        reranked = self.rerank_candidates(query, all_candidates)\n        \n        if log_diagnostics:\n            logger.info(\"Top reranked contexts:\")\n            for i, (text, score, domain) in enumerate(reranked[:3]):\n                logger.info(f\"  {i+1}. [{domain}] (score={score:.3f}): {text[:150]}...\")\n        \n        answer = self.generate_answer(query, reranked)\n        metrics = self.compute_metrics(query, answer, reranked)\n        \n        result = {\n            \"query\": query,\n            \"routed_domains\": selected_domains,\n            \"answer\": answer,\n            \"contexts\": [(text, domain) for text, _, domain in reranked[:self.config.context_chunks]],\n            \"metrics\": metrics\n        }\n        \n        return result\n    \n    def evaluate_batch(self, queries: List[str], log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"üìä Evaluating {len(queries)} queries...\")\n        \n        results = []\n        failed = []\n        \n        for i, query in enumerate(queries):\n            try:\n                result = self.run_query(query, log_diagnostics=log_diagnostics)\n                results.append(result)\n                \n                if (i + 1) % 3 == 0:\n                    logger.info(f\"Progress: {i+1}/{len(queries)}\")\n                    monitor_memory()\n            except Exception as e:\n                logger.error(f\"Failed query {i}: {e}\")\n                failed.append((i, query, str(e)))\n        \n        if not results:\n            return {\"error\": \"No successful queries\"}\n        \n        avg_metrics = {\n            \"retrieval\": np.mean([r[\"metrics\"][\"retrieval\"] for r in results]),\n            \"completeness\": np.mean([r[\"metrics\"][\"completeness\"] for r in results]),\n            \"faithfulness\": np.mean([r[\"metrics\"][\"faithfulness\"] for r in results]),\n            \"composite\": np.mean([r[\"metrics\"][\"composite\"] for r in results])\n        }\n        \n        summary = {\n            \"total_queries\": len(queries),\n            \"successful\": len(results),\n            \"failed\": len(failed),\n            \"success_rate\": len(results) / len(queries),\n            \"average_metrics\": avg_metrics,\n            \"failed_queries\": failed,\n            \"individual_results\": results\n        }\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_file = f\"evaluation_{timestamp}.json\"\n        try:\n            with open(results_file, \"w\") as f:\n                json.dump(summary, f, indent=2, default=str)\n            logger.info(f\"üíæ Results saved to {results_file}\")\n        except:\n            pass\n        \n        return summary\n\n# ============================================================================\n# HELPER FUNCTIONS FOR EXECUTION\n# ============================================================================\n\ndef load_queries_from_file(file_path: str) -> List[str]:\n    \"\"\"Load queries from a text file (one query per line).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            queries = [line.strip() for line in f if line.strip()]\n        logger.info(f\"‚úÖ Loaded {len(queries)} queries from {file_path}\")\n        return queries\n    except Exception as e:\n        logger.error(f\"‚ùå Failed to load queries from {file_path}: {e}\")\n        return []\n\ndef run_interactive_mode(pipeline):\n    \"\"\"Interactive mode: user enters queries one by one.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üéØ INTERACTIVE QUERY MODE\")\n    print(\"=\"*80)\n    print(\"Enter your medical queries (type 'quit' or 'exit' to stop):\\n\")\n    \n    while True:\n        query = input(\"Query: \").strip()\n        if query.lower() in ['quit', 'exit', 'q']:\n            print(\"üëã Exiting interactive mode.\")\n            break\n        if not query:\n            continue\n            \n        result = pipeline.run_query(query, top_domains=2, log_diagnostics=False)\n        print(\"\\n\" + \"-\"*80)\n        print(f\"QUERY: {result['query']}\")\n        print(f\"Routed to: {result['routed_domains']}\")\n        print(f\"Answer: {result['answer']}\")\n        print(\"\\nüìä METRICS:\")\n        for metric_name, value in result['metrics'].items():\n            print(f\"  {metric_name}: {value:.3f}\")\n        print(\"-\"*80 + \"\\n\")\n\ndef run_batch_from_list(pipeline, queries: List[str], show_individual: bool = True):\n    \"\"\"Run batch evaluation on a list of queries.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"üìä BATCH EVALUATION MODE ({len(queries)} queries)\")\n    print(\"=\"*80 + \"\\n\")\n    \n    if show_individual:\n        for i, query in enumerate(queries, 1):\n            print(f\"\\n[Query {i}/{len(queries)}]\")\n            result = pipeline.run_query(query, top_domains=2, log_diagnostics=False)\n            print(f\"Q: {result['query']}\")\n            print(f\"Routed to: {result['routed_domains']}\")\n            print(f\"A: {result['answer']}\")\n            print(\"Metrics:\", end=\" \")\n            for metric_name, value in result['metrics'].items():\n                print(f\"{metric_name}={value:.3f}\", end=\" \")\n            print(\"\\n\" + \"-\"*80)\n    \n    batch_results = pipeline.evaluate_batch(queries, log_diagnostics=False)\n    print(\"\\nüìà BATCH EVALUATION SUMMARY\")\n    print(\"=\"*80)\n    print(f\"Success Rate: {batch_results['success_rate']:.1%}\")\n    print(f\"Average Retrieval: {batch_results['average_metrics']['retrieval']:.3f}\")\n    print(f\"Average Completeness: {batch_results['average_metrics']['completeness']:.3f}\")\n    print(f\"Average Faithfulness: {batch_results['average_metrics']['faithfulness']:.3f}\")\n    print(f\"Average Composite: {batch_results['average_metrics']['composite']:.3f}\")\n    print(\"=\"*80)\n    \n    return batch_results\n\nprint(\"=\"*80)\nprint(\"üöÄ INITIALIZING MULTI-DOMAIN RAG PIPELINE\")\nprint(\"=\"*80)\n\nrag_pipeline = MultiDomainRAGPipeline(config, DOMAINS)\n\nprint(\"\\n‚úÖ Pipeline ready! Run the next cell to execute queries.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T05:18:37.229773Z","iopub.execute_input":"2025-11-01T05:18:37.230287Z","iopub.status.idle":"2025-11-01T05:26:13.508149Z","shell.execute_reply.started":"2025-11-01T05:18:37.230268Z","shell.execute_reply":"2025-11-01T05:26:13.507277Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüöÄ INITIALIZING MULTI-DOMAIN RAG PIPELINE\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8420618889054b129d46eab4f588468b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"640ff7d2b42a4014808ce26b525b3693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448abd52b1cd4a1184d0b982dfd0b1cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b373256a384c7b9d0786d5c2b8df14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ab55bc20f24f6f8e4f483b3f0c4e99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca83c48f4c794b51a07575a59c515b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acbde7df8b02467b81b34d48904db6d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a936a49fe153498d8e331b5833039272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758a4091ff874614bc65c8b91307a57a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf477b49621f4debad598c70d581ca8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72eb7a182c204a7aa7fc3ec63d0c2a41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853743f49f2d48e0ac00eb6043c66f99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb32971244a498f9edb26f2afa3eb3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff135a149cee4eba841bd816462aeae4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08a20198a2e1412485d7bae7cded1642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0cf6da10344859a66fd7f50df4093d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d985d56cf56484b8602d907abebf72f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"518cb0f4a7c94528aeb82da6f6758c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728b1f9e845245c798848843622d9565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43b09687a80a47b09bcb2fbd397e9040"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610a086be8364235bd84a9189f9d409d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394a24cf0bde4417904bd511230680c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc714f650e34449815a935e5e98192c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"052d33c515444bb0af6244a09f364d9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a4e23a436a45749a2d849c70063c40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43f772c1c85e4249899fbe28665319c1"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"women-health-mini.jsonl:   0%|          | 0.00/35.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3a274293f7a48589ec9fbd24ab5cab8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10348 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e77501a7b3457bbb7adb7903975a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d40e6462974051a61a0631df836191"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all-processed/train-00000-of-00001-9bfe4(‚Ä¶):   0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43d9fa1e38134b8892ddc64fc4652e00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/246678 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ad7cd96c043416aa4141f2ba6f42299"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da44b1f62295414faab6cf589653065b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3c0f85360b4c47b4c456aa24a8d337"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Pipeline ready! Run the next cell to execute queries.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: EXECUTION MODES - CHANGE FLAGS AND RERUN THIS CELL\n# ============================================================================\n\n# Mode 1: Interactive (user input)\nRUN_INTERACTIVE = True  \n\n# Mode 2: Batch from file\nRUN_FROM_FILE = False\nQUERIES_FILE = \"queries.txt\"\n\n# Mode 3: Batch from predefined list\nRUN_DEMO_BATCH = False\nDEMO_QUERIES = [\n    \"What are the recommended health screenings for women in their 40s?\",\n    \"I'm 35 years old with irregular periods and excessive facial hair. What condition might this indicate, what tests should I get, and what are the treatment options?\",\n    \"How does gestational diabetes differ from Type 2 diabetes, and what are the specific screening guidelines for pregnant women?\"\n]\n\n# Mode 4: Single query demo\nRUN_SINGLE_DEMO = False\nSINGLE_QUERY = \"What are the symptoms of menopause?\"\n\n# ========================================================================\n# EXECUTE\n# ========================================================================\n\nif RUN_INTERACTIVE:\n    run_interactive_mode(rag_pipeline)\n\nelif RUN_FROM_FILE:\n    queries = load_queries_from_file(QUERIES_FILE)\n    if queries:\n        batch_results = run_batch_from_list(rag_pipeline, queries, show_individual=True)\n\nelif RUN_DEMO_BATCH:\n    batch_results = run_batch_from_list(rag_pipeline, DEMO_QUERIES, show_individual=True)\n\nelif RUN_SINGLE_DEMO:\n    result = rag_pipeline.run_query(SINGLE_QUERY, top_domains=2, log_diagnostics=True)\n    print(\"\\n==================\")\n    print(f\"QUERY: {result['query']}\")\n    print(\"==================\")\n    print(f\"Routed to: {result['routed_domains']}\")\n    print(f\"Answer: {result['answer']}\")\n    print(\"\\nüìä METRICS:\")\n    for metric_name, value in result['metrics'].items():\n        print(f\"  {metric_name}: {value:.3f}\")\n\nelse:\n    print(\"‚ö†Ô∏è No execution mode selected. Set one of the RUN_* flags to True.\")\n\ntry:\n    with open(config.prompts_log, \"wb\") as f:\n        pickle.dump(rag_pipeline.prompts_log, f)\n    print(f\"\\nüìù Prompt logs saved to {config.prompts_log}\")\nexcept Exception as e:\n    logger.warning(f\"Could not save prompt logs: {e}\")\n\nprint(\"\\n‚úÖ EXECUTION COMPLETE\")\nmonitor_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T05:26:13.508947Z","iopub.execute_input":"2025-11-01T05:26:13.509163Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüéØ INTERACTIVE QUERY MODE\n================================================================================\nEnter your medical queries (type 'quit' or 'exit' to stop):\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Query:  what is neurology\n"},{"name":"stdout","text":"\n--------------------------------------------------------------------------------\nQUERY: what is neurology\nRouted to: []\nAnswer: I apologize, but I'm designed to answer medical and health-related questions. Your question doesn't appear to be medical in nature. Could you please ask a health or medical question?\n\nüìä METRICS:\n  retrieval: 0.000\n  completeness: 0.000\n  faithfulness: 0.000\n  composite: 0.000\n--------------------------------------------------------------------------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Query:  im ill,what medication should i tale\n"},{"name":"stdout","text":"\n--------------------------------------------------------------------------------\nQUERY: im ill,what medication should i tale\nRouted to: []\nAnswer: I apologize, but I'm designed to answer medical and health-related questions. Your question doesn't appear to be medical in nature. Could you please ask a health or medical question?\n\nüìä METRICS:\n  retrieval: 0.000\n  completeness: 0.000\n  faithfulness: 0.000\n  composite: 0.000\n--------------------------------------------------------------------------------\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}