{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL 1: FIX DEPENDENCIES - RUN FIRST AFTER RESTART\n# ============================================================================\n\nimport subprocess\nimport sys\n\nprint('ğŸ”§ Fixing package dependencies...')\n\n# Fix PyArrow compatibility - install version 15.0.2 for bigframes\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"], \n                capture_output=True, check=False)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow==15.0.2\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Fix rich version for bigframes compatibility\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rich==13.7.1\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Install google-cloud-bigquery-storage (missing dependency)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-bigquery-storage>=2.30.0\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Upgrade google-cloud-bigquery for bigframes\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"google-cloud-bigquery>=3.31.0\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Upgrade google-api-core for pandas-gbq\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"google-api-core>=2.10.2\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Install missing packages including faiss-cpu and fix protobuf\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"keybert\", \"rank-bm25\", \"evaluate\", \"faiss-cpu\", \"protobuf<5.0.0\"], \n                check=True)\n\nprint('âœ… Dependencies fixed and installed')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T18:21:02.690265Z","iopub.execute_input":"2025-10-24T18:21:02.691068Z","iopub.status.idle":"2025-10-24T18:21:29.139045Z","shell.execute_reply.started":"2025-10-24T18:21:02.691040Z","shell.execute_reply":"2025-10-24T18:21:29.138158Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Fixing package dependencies...\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 38.3/38.3 MB 252.4 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31.4/31.4 MB 57.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 294.9/294.9 kB 20.1 MB/s eta 0:00:00\nâœ… Dependencies fixed and installed\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngrpcio-status 1.76.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 4.25.8 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.27.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: VERIFY ALL IMPORTS - RUN SECOND\n# ============================================================================\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"ğŸ” Testing imports...\")\n\ntry:\n    from datasets import load_dataset\n    print(\"âœ… datasets\")\n    from sentence_transformers import SentenceTransformer\n    print(\"âœ… sentence-transformers\")\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    print(\"âœ… transformers\")\n    import faiss\n    print(\"âœ… faiss\")\n    from keybert import KeyBERT\n    print(\"âœ… keybert\")\n    from rank_bm25 import BM25Okapi\n    print(\"âœ… rank-bm25\")\n    import torch\n    print(f\"âœ… torch (device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n    print(\"\\nğŸ‰ ALL IMPORTS SUCCESSFUL!\")\nexcept Exception as e:\n    print(f\"âŒ Import failed: {e}\\nPlease restart kernel and try again.\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T18:21:29.140512Z","iopub.execute_input":"2025-10-24T18:21:29.140787Z","iopub.status.idle":"2025-10-24T18:21:39.689271Z","shell.execute_reply.started":"2025-10-24T18:21:29.140767Z","shell.execute_reply":"2025-10-24T18:21:39.688561Z"}},"outputs":[{"name":"stdout","text":"ğŸ” Testing imports...\nâœ… datasets\n","output_type":"stream"},{"name":"stderr","text":"2025-10-24 18:21:35.362382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761330095.384307     194 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761330095.391090     194 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"âœ… sentence-transformers\nâœ… transformers\nâœ… faiss\nâœ… keybert\nâœ… rank-bm25\nâœ… torch (device: cuda)\n\nğŸ‰ ALL IMPORTS SUCCESSFUL!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# MULTI-DOMAIN RAG PIPELINE FOR MEDICAL QA\n# Complete production-ready implementation\n# Datasets: Women's Health + General Medical QA\n# Models: all-MiniLM (embedder), BGE-reranker, BioGPT (HyDE), Flan-T5 (generator)\n# ============================================================================\n\nimport os, sys, time, json, pickle, re, warnings, random\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport torch\nfrom pathlib import Path\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Optional, Any, Tuple\nimport logging\nfrom datetime import datetime\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\nfrom keybert import KeyBERT\nimport faiss\nfrom rank_bm25 import BM25Okapi\n\n# ============================================================================\n# REPRODUCIBILITY & SETUP\n# ============================================================================\n\ndef set_all_seeds(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_all_seeds(42)\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Download NLTK data\nfor resource in ['punkt', 'stopwords']:\n    try:\n        nltk.data.find(f'tokenizers/{resource}')\n    except LookupError:\n        nltk.download(resource, quiet=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"ğŸš€ Device: {device}\")\nif torch.cuda.is_available():\n    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}, Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n@dataclass\nclass DomainConfig:\n    name: str\n    dataset_name: str\n    dataset_split: str = \"train\"\n    index_path: str = None\n    id2doc_path: str = None\n    metadata_path: str = None\n    \n    def __post_init__(self):\n        if self.index_path is None:\n            self.index_path = f\"{self.name}_faiss.index\"\n        if self.id2doc_path is None:\n            self.id2doc_path = f\"{self.name}_id2doc.pkl\"\n        if self.metadata_path is None:\n            self.metadata_path = f\"{self.name}_metadata.json\"\n\n@dataclass\nclass RAGConfig:\n    embed_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    reranker_model: str = \"BAAI/bge-reranker-large\"\n    hyde_model: str = \"microsoft/BioGPT-Large\"\n    generator_model: str = \"google/flan-t5-large\"\n    \n    chunk_window: int = 3\n    chunk_stride: int = 1\n    retrieve_k: int = 30\n    rerank_topk: int = 8\n    context_chunks: int = 4\n    hyde_weight: float = 0.4\n    faiss_alpha: float = 0.6\n    \n    max_new_tokens: int = 200\n    hyde_max_tokens: int = 60\n    \n    completeness_threshold: float = 0.65\n    faithfulness_threshold: float = 0.55\n    \n    retrieval_weight: float = 0.4\n    completeness_weight: float = 0.3\n    faithfulness_weight: float = 0.3\n    \n    prompts_log: str = \"prompts_outputs.pkl\"\n    random_seed: int = 42\n    test_size: float = 0.15\n\nDOMAINS = [\n    DomainConfig(name=\"women_health\", dataset_name=\"altaidevorg/women-health-mini\"),\n    DomainConfig(name=\"medical_qa\", dataset_name=\"Malikeh1375/medical-question-answering-datasets\")\n]\n\nconfig = RAGConfig()\n\n# ============================================================================\n# UTILITIES\n# ============================================================================\n\ndef clean_text_artifacts(text: str) -> str:\n    text = re.sub(r\"^(Answer:|Final answer:|Response:)\\s*\", \"\", text, flags=re.IGNORECASE)\n    text = re.sub(r\"<\\/?[^>]+>|</s>|â–ƒ|\\[INST\\]|\\[/INST\\]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text.strip(\" \\n\\r\\t\\\"'\")\n\ndef monitor_memory():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        logger.info(f\"ğŸ’¾ GPU: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n        if allocated/total > 0.85:\n            torch.cuda.empty_cache()\n\n# ============================================================================\n# DATA LOADING\n# ============================================================================\n\nclass DatasetLoader:\n    @staticmethod\n    def extract_qa_pairs(dataset, domain_name: str) -> List[Dict[str, Any]]:\n        qa_data = []\n        for idx, row in enumerate(dataset):\n            try:\n                conv = None\n                if isinstance(row, dict):\n                    for field in [\"conversations\", \"conversation\", \"dialog\", \"dialogue\", \"messages\", \"turns\"]:\n                        if field in row:\n                            conv = row[field]\n                            break\n                \n                if not conv:\n                    continue\n                \n                user_msgs, assistant_msgs = [], []\n                \n                if isinstance(conv, list) and len(conv) > 0:\n                    if isinstance(conv[0], dict):\n                        if \"from\" in conv[0] and \"value\" in conv[0]:\n                            user_msgs = [m[\"value\"] for m in conv if m.get(\"from\") in (\"human\", \"user\")]\n                            assistant_msgs = [m[\"value\"] for m in conv if m.get(\"from\") in (\"assistant\", \"bot\", \"system\")]\n                        elif \"role\" in conv[0] and \"content\" in conv[0]:\n                            user_msgs = [m[\"content\"] for m in conv if m.get(\"role\") in (\"user\", \"human\")]\n                            assistant_msgs = [m[\"content\"] for m in conv if m.get(\"role\") in (\"assistant\", \"bot\")]\n                    else:\n                        if len(conv) >= 2:\n                            user_msgs, assistant_msgs = [conv[0]], conv[1:]\n                \n                if user_msgs and assistant_msgs:\n                    question = \" \".join(user_msgs).strip()\n                    answer = \" \".join(assistant_msgs).strip()\n                    if question and answer and len(question) > 10 and len(answer) > 10:\n                        qa_data.append({\n                            \"question\": question,\n                            \"answer\": answer,\n                            \"domain\": domain_name,\n                            \"source_id\": idx\n                        })\n            except Exception:\n                continue\n        return qa_data\n    \n    @staticmethod\n    def load_domain_data(domain_config: DomainConfig) -> Tuple[List[Dict], List[Dict]]:\n        logger.info(f\"ğŸ“¥ Loading {domain_config.name}...\")\n        try:\n            dataset = load_dataset(domain_config.dataset_name, split=domain_config.dataset_split)\n            qa_data = DatasetLoader.extract_qa_pairs(dataset, domain_config.name)\n            \n            if not qa_data:\n                raise ValueError(f\"No QA pairs extracted\")\n            \n            train_data, test_data = train_test_split(\n                qa_data, test_size=config.test_size, random_state=config.random_seed\n            )\n            logger.info(f\"âœ… {domain_config.name}: {len(train_data)} train, {len(test_data)} test\")\n            return train_data, test_data\n        except Exception as e:\n            logger.error(f\"âŒ Failed to load {domain_config.name}: {e}\")\n            raise\n\n# ============================================================================\n# TEXT CHUNKING\n# ============================================================================\n\nclass TextChunker:\n    @staticmethod\n    def create_chunks(data: List[Dict], window: int = 3, stride: int = 1, min_chars: int = 50) -> List[Dict]:\n        chunks = []\n        for item in data:\n            text = item.get(\"answer\", \"\")\n            if not text or len(text) < min_chars:\n                continue\n            \n            sentences = sent_tokenize(text)\n            if not sentences:\n                continue\n            \n            if len(sentences) <= window:\n                chunks.append({\n                    \"chunk\": \" \".join(sentences),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks)\n                })\n                continue\n            \n            for i in range(0, max(1, len(sentences) - window + 1), stride):\n                chunks.append({\n                    \"chunk\": \" \".join(sentences[i:i + window]),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks),\n                    \"window\": (i, i + window)\n                })\n        return chunks\n\n# ============================================================================\n# MODEL MANAGEMENT\n# ============================================================================\n\nclass ModelManager:\n    def __init__(self, config: RAGConfig, device: torch.device):\n        self.config = config\n        self.device = device\n        self.models = {}\n    \n    def load_embedder(self):\n        logger.info(f\"ğŸ“¦ Loading embedder...\")\n        embedder = SentenceTransformer(self.config.embed_model, device=self.device)\n        self.models['embedder'] = embedder\n        logger.info(f\"âœ… Embedder loaded\")\n        return embedder\n    \n    def load_reranker(self):\n        logger.info(f\"ğŸ“¦ Loading reranker...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.reranker_model)\n        model = AutoModelForSequenceClassification.from_pretrained(\n            self.config.reranker_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        ).to(self.device)\n        model.eval()\n        self.models['reranker_tokenizer'] = tokenizer\n        self.models['reranker_model'] = model\n        logger.info(f\"âœ… Reranker loaded\")\n        return tokenizer, model\n    \n    def load_hyde_model(self):\n        logger.info(f\"ğŸ“¦ Loading HyDE model...\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(self.config.hyde_model)\n            model = AutoModelForCausalLM.from_pretrained(\n                self.config.hyde_model,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                low_cpu_mem_usage=True\n            ).to(self.device)\n            model.eval()\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            self.models['hyde_tokenizer'] = tokenizer\n            self.models['hyde_model'] = model\n            logger.info(f\"âœ… HyDE model loaded\")\n            return tokenizer, model\n        except Exception as e:\n            logger.warning(f\"âš ï¸ HyDE load failed, using query expansion: {e}\")\n            return None, None\n    \n    def load_generator(self):\n        logger.info(f\"ğŸ“¦ Loading generator...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.generator_model)\n        model = AutoModelForCausalLM.from_pretrained(\n            self.config.generator_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            low_cpu_mem_usage=True\n        ).to(self.device)\n        model.eval()\n        self.models['gen_tokenizer'] = tokenizer\n        self.models['gen_model'] = model\n        logger.info(f\"âœ… Generator loaded\")\n        return tokenizer, model\n    \n    def load_keyword_extractor(self):\n        try:\n            kw_model = KeyBERT(model=self.models.get('embedder'))\n            self.models['keyword_extractor'] = kw_model\n            logger.info(f\"âœ… KeyBERT loaded\")\n            return kw_model\n        except Exception as e:\n            logger.warning(f\"âš ï¸ KeyBERT load failed: {e}\")\n            return None\n    \n    def load_all(self):\n        logger.info(\"ğŸ”§ Loading all models...\")\n        self.load_embedder()\n        self.load_reranker()\n        self.load_hyde_model()\n        self.load_generator()\n        self.load_keyword_extractor()\n        monitor_memory()\n        logger.info(\"âœ… All models loaded\")\n        return self.models\n\n# ============================================================================\n# INDEX MANAGEMENT\n# ============================================================================\n\nclass MultiDomainIndexManager:\n    def __init__(self, config: RAGConfig, embedder: SentenceTransformer):\n        self.config = config\n        self.embedder = embedder\n        self.domain_indices = {}\n    \n    def build_or_load_domain_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        if Path(domain_config.index_path).exists() and Path(domain_config.id2doc_path).exists():\n            try:\n                return self._load_existing_index(domain_config)\n            except:\n                pass\n        return self._build_new_index(domain_config, chunks)\n    \n    def _load_existing_index(self, domain_config: DomainConfig) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"ğŸ“‚ Loading existing {domain_config.name} index...\")\n        index = faiss.read_index(domain_config.index_path)\n        with open(domain_config.id2doc_path, \"rb\") as f:\n            id2doc = pickle.load(f)\n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        logger.info(f\"âœ… Loaded {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def _build_new_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"ğŸ”¨ Building {domain_config.name} index...\")\n        id2doc = [chunk[\"chunk\"] for chunk in chunks]\n        \n        embeddings = self.embedder.encode(\n            id2doc, normalize_embeddings=True, show_progress_bar=True,\n            batch_size=64, convert_to_numpy=True\n        ).astype('float32')\n        \n        dim = embeddings.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(embeddings)\n        \n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        \n        faiss.write_index(index, domain_config.index_path)\n        with open(domain_config.id2doc_path, \"wb\") as f:\n            pickle.dump(id2doc, f)\n        \n        metadata = {\"created_at\": time.time(), \"n_vectors\": int(index.ntotal), \"embedding_dim\": dim, \"domain\": domain_config.name}\n        with open(domain_config.metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n        \n        logger.info(f\"âœ… Built {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def load_all_domains(self, domain_chunks: Dict[str, List[Dict]]):\n        for domain in DOMAINS:\n            index, id2doc, bm25 = self.build_or_load_domain_index(domain, domain_chunks.get(domain.name, []))\n            self.domain_indices[domain.name] = {\n                'index': index, 'id2doc': id2doc, 'bm25': bm25, 'config': domain\n            }\n        logger.info(f\"âœ… Loaded {len(self.domain_indices)} domain indices\")\n\n# ============================================================================\n# QUERY ROUTER\n# ============================================================================\n\nclass QueryRouter:\n    def __init__(self, embedder: SentenceTransformer, domain_indices: Dict):\n        self.embedder = embedder\n        self.domain_indices = domain_indices\n        self.domain_centroids = self._compute_centroids()\n    \n    def _compute_centroids(self) -> Dict[str, np.ndarray]:\n        centroids = {}\n        logger.info(\"ğŸ¯ Computing domain centroids...\")\n        for domain_name, domain_data in self.domain_indices.items():\n            id2doc = domain_data['id2doc']\n            sample_docs = random.sample(id2doc, min(500, len(id2doc)))\n            embeddings = self.embedder.encode(sample_docs, normalize_embeddings=True, convert_to_numpy=True)\n            centroids[domain_name] = embeddings.mean(axis=0)\n        return centroids\n    \n    def route_query(self, query: str, top_k: int = 2) -> List[str]:\n        query_emb = self.embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n        similarities = {domain: float(np.dot(query_emb, centroid)) for domain, centroid in self.domain_centroids.items()}\n        sorted_domains = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        selected = [d[0] for d in sorted_domains[:top_k]]\n        logger.info(f\"ğŸ§­ Routed to: {selected}\")\n        return selected\n\n# ============================================================================\n# RAG PIPELINE\n# ============================================================================\n\nclass MultiDomainRAGPipeline:\n    def __init__(self, config: RAGConfig, domains: List[DomainConfig]):\n        self.config = config\n        self.domains = domains\n        self.device = device\n        \n        self.model_manager = ModelManager(config, device)\n        self.models = self.model_manager.load_all()\n        \n        self.data = {}\n        self.test_data = {}\n        domain_chunks = {}\n        \n        for domain in domains:\n            train_data, test_data = DatasetLoader.load_domain_data(domain)\n            self.data[domain.name] = train_data\n            self.test_data[domain.name] = test_data\n            chunks = TextChunker.create_chunks(train_data, window=config.chunk_window, stride=config.chunk_stride)\n            domain_chunks[domain.name] = chunks\n        \n        self.index_manager = MultiDomainIndexManager(config, self.models['embedder'])\n        self.index_manager.load_all_domains(domain_chunks)\n        \n        self.router = QueryRouter(self.models['embedder'], self.index_manager.domain_indices)\n        self.prompts_log = []\n        \n        logger.info(\"âœ… Multi-domain RAG pipeline initialized\")\n    \n    def generate_hyde_answer(self, query: str) -> str:\n        if self.models['hyde_model'] is None:\n            return query\n        \n        prompt = f\"Question: {query}\\nAnswer:\"\n        try:\n            inputs = self.models['hyde_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['hyde_model'].generate(\n                    **inputs, max_new_tokens=self.config.hyde_max_tokens,\n                    do_sample=False, pad_token_id=self.models['hyde_tokenizer'].eos_token_id,\n                    repetition_penalty=1.15\n                )\n            text = self.models['hyde_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            hyde = clean_text_artifacts(text.split(\"Answer:\")[-1])\n            return hyde if hyde else query\n        except:\n            return query\n    \n    def retrieve_from_domain(self, query: str, domain_name: str, k: int) -> List[Tuple[int, float, str]]:\n        domain_data = self.index_manager.domain_indices[domain_name]\n        index = domain_data['index']\n        id2doc = domain_data['id2doc']\n        bm25 = domain_data['bm25']\n        \n        hyde_text = self.generate_hyde_answer(query)\n        q_emb = self.models['embedder'].encode([query], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        h_emb = self.models['embedder'].encode([hyde_text], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        merged_emb = (1 - self.config.hyde_weight) * q_emb + self.config.hyde_weight * h_emb\n        \n        D, I = index.search(merged_emb, k)\n        faiss_scores = D[0]\n        if faiss_scores.max() > faiss_scores.min():\n            faiss_norm = (faiss_scores - faiss_scores.min()) / (faiss_scores.max() - faiss_scores.min())\n        else:\n            faiss_norm = np.ones_like(faiss_scores)\n        faiss_map = {int(idx): float(score) for idx, score in zip(I[0], faiss_norm)}\n        \n        bm25_scores = bm25.get_scores(word_tokenize(query.lower()))\n        if bm25_scores.max() > bm25_scores.min():\n            bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())\n        else:\n            bm25_norm = np.zeros_like(bm25_scores)\n        \n        candidates = set(I[0].tolist()) | set(np.argsort(bm25_scores)[::-1][:k].tolist())\n        merged_scores = []\n        for idx in candidates:\n            f = faiss_map.get(int(idx), 0.0)\n            b = float(bm25_norm[int(idx)]) if int(idx) < len(bm25_norm) else 0.0\n            score = self.config.faiss_alpha * f + (1 - self.config.faiss_alpha) * b\n            merged_scores.append((int(idx), score, domain_name))\n        \n        merged_scores.sort(key=lambda x: x[1], reverse=True)\n        return merged_scores[:k]\n    \n    def rerank_candidates(self, query: str, candidates: List[Tuple[int, float, str]]) -> List[Tuple[str, float, str]]:\n        texts, metadata = [], []\n        for idx, score, domain_name in candidates:\n            domain_data = self.index_manager.domain_indices[domain_name]\n            text = domain_data['id2doc'][idx]\n            texts.append(text)\n            metadata.append((idx, domain_name))\n        \n        reranker_scores = []\n        batch_size = 8\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = self.models['reranker_tokenizer'](\n                [query] * len(batch_texts), batch_texts,\n                padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n            ).to(self.device)\n            \n            with torch.no_grad():\n                outputs = self.models['reranker_model'](**inputs)\n                logits = outputs.logits.cpu().numpy()\n            \n            for lg in logits:\n                if lg.shape == ():\n                    score = float(lg)\n                elif len(lg.shape) == 1 and lg.shape[0] == 1:\n                    score = float(lg[0])\n                elif len(lg.shape) == 1 and lg.shape[0] == 2:\n                    score = float(lg[1])\n                else:\n                    score = float(np.max(lg))\n                reranker_scores.append(score)\n        \n        reranked = [(texts[i], reranker_scores[i], metadata[i][1]) for i in range(len(texts))]\n        reranked.sort(key=lambda x: x[1], reverse=True)\n        return reranked[:self.config.rerank_topk]\n    \n    def generate_answer(self, query: str, contexts: List[Tuple[str, float, str]]) -> str:\n        context_parts = [f\"[Source {i+1} from {domain}]:\\n{text}\" \n                        for i, (text, score, domain) in enumerate(contexts[:self.config.context_chunks])]\n        context_block = \"\\n\\n\".join(context_parts)\n        \n        prompt = f\"\"\"Based on the following medical information, answer the question concisely and accurately.\n\n{context_block}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n        \n        try:\n            inputs = self.models['gen_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['gen_model'].generate(\n                    **inputs, max_new_tokens=self.config.max_new_tokens,\n                    do_sample=False, pad_token_id=self.models['gen_tokenizer'].eos_token_id,\n                    repetition_penalty=1.1\n                )\n            raw = self.models['gen_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            answer = clean_text_artifacts(raw.split(\"Answer:\")[-1])\n            \n            self.prompts_log.append({\n                \"type\": \"generate\", \"query\": query,\n                \"contexts\": [(t, d) for t, _, d in contexts[:self.config.context_chunks]],\n                \"prompt\": prompt, \"raw\": raw, \"answer\": answer, \"timestamp\": time.time()\n            })\n            \n            return answer if answer else \"Insufficient information.\"\n        except Exception as e:\n            logger.error(f\"Generation failed: {e}\")\n            return \"Error generating answer.\"\n    \n    def compute_metrics(self, query: str, answer: str, contexts: List[Tuple[str, float, str]]) -> Dict[str, float]:\n        metrics = {}\n        \n        if contexts:\n            retrieval_score = np.mean([score for _, score, _ in contexts[:self.config.context_chunks]])\n            metrics['retrieval'] = float(retrieval_score)\n        else:\n            metrics['retrieval'] = 0.0\n        \n        try:\n            context_texts = [text for text, _, _ in contexts[:self.config.context_chunks]]\n            all_keywords = []\n            \n            if self.models['keyword_extractor']:\n                for ctx_text in context_texts:\n                    keywords = self.models['keyword_extractor'].extract_keywords(\n                        ctx_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5\n                    )\n                    all_keywords.extend([kw for kw, _ in keywords])\n            \n            unique_keywords = list(dict.fromkeys([kw.lower() for kw in all_keywords if kw]))\n            \n            if unique_keywords and answer:\n                answer_emb = self.models['embedder'].encode([answer], normalize_embeddings=True, convert_to_tensor=True)\n                keyword_embs = self.models['embedder'].encode(unique_keywords, normalize_embeddings=True, convert_to_tensor=True)\n                similarities = util.cos_sim(answer_emb, keyword_embs).cpu().numpy()[0]\n                covered = (similarities >= self.config.completeness_threshold).sum()\n                metrics['completeness'] = float(covered / len(unique_keywords))\n            else:\n                metrics['completeness'] = 0.0\n        except:\n            metrics['completeness'] = 0.0\n        \n        try:\n            if answer and contexts:\n                answer_sentences = sent_tokenize(answer)\n                context_sentences = []\n                for text, _, _ in contexts[:self.config.context_chunks]:\n                    context_sentences.extend(sent_tokenize(text))\n                \n                if answer_sentences and context_sentences:\n                    ans_embs = self.models['embedder'].encode(answer_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    ctx_embs = self.models['embedder'].encode(context_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    sim_matrix = util.cos_sim(ans_embs, ctx_embs).cpu().numpy()\n                    max_sims = np.max(sim_matrix, axis=1)\n                    faithful = (max_sims >= self.config.faithfulness_threshold).sum()\n                    metrics['faithfulness'] = float(faithful / len(answer_sentences))\n                else:\n                    metrics['faithfulness'] = 0.0\n            else:\n                metrics['faithfulness'] = 0.0\n        except:\n            metrics['faithfulness'] = 0.0\n        \n        metrics['composite'] = (\n            self.config.retrieval_weight * metrics['retrieval'] +\n            self.config.completeness_weight * metrics['completeness'] +\n            self.config.faithfulness_weight * metrics['faithfulness']\n        )\n        \n        return metrics\n    \n    def run_query(self, query: str, top_domains: int = 2, log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"ğŸ” Processing: {query[:100]}...\")\n        \n        selected_domains = self.router.route_query(query, top_k=top_domains)\n        \n        all_candidates = []\n        for domain_name in selected_domains:\n            candidates = self.retrieve_from_domain(query, domain_name, k=self.config.retrieve_k)\n            all_candidates.extend(candidates)\n        \n        if log_diagnostics:\n            logger.info(f\"Retrieved {len(all_candidates)} candidates from {len(selected_domains)} domains\")\n        \n        reranked = self.rerank_candidates(query, all_candidates)\n        \n        if log_diagnostics:\n            logger.info(\"Top reranked contexts:\")\n            for i, (text, score, domain) in enumerate(reranked[:3]):\n                logger.info(f\"  {i+1}. [{domain}] (score={score:.3f}): {text[:150]}...\")\n        \n        answer = self.generate_answer(query, reranked)\n        metrics = self.compute_metrics(query, answer, reranked)\n        \n        result = {\n            \"query\": query,\n            \"routed_domains\": selected_domains,\n            \"answer\": answer,\n            \"contexts\": [(text, domain) for text, _, domain in reranked[:self.config.context_chunks]],\n            \"metrics\": metrics\n        }\n        \n        return result\n    \n    def evaluate_batch(self, queries: List[str], log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"ğŸ“Š Evaluating {len(queries)} queries...\")\n        \n        results = []\n        failed = []\n        \n        for i, query in enumerate(queries):\n            try:\n                result = self.run_query(query, log_diagnostics=log_diagnostics)\n                results.append(result)\n                \n                if (i + 1) % 3 == 0:\n                    logger.info(f\"Progress: {i+1}/{len(queries)}\")\n                    monitor_memory()\n            except Exception as e:\n                logger.error(f\"Failed query {i}: {e}\")\n                failed.append((i, query, str(e)))\n        \n        if not results:\n            return {\"error\": \"No successful queries\"}\n        \n        avg_metrics = {\n            \"retrieval\": np.mean([r[\"metrics\"][\"retrieval\"] for r in results]),\n            \"completeness\": np.mean([r[\"metrics\"][\"completeness\"] for r in results]),\n            \"faithfulness\": np.mean([r[\"metrics\"][\"faithfulness\"] for r in results]),\n            \"composite\": np.mean([r[\"metrics\"][\"composite\"] for r in results])\n        }\n        \n        summary = {\n            \"total_queries\": len(queries),\n            \"successful\": len(results),\n            \"failed\": len(failed),\n            \"success_rate\": len(results) / len(queries),\n            \"average_metrics\": avg_metrics,\n            \"failed_queries\": failed,\n            \"individual_results\": results\n        }\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_file = f\"evaluation_{timestamp}.json\"\n        try:\n            with open(results_file, \"w\") as f:\n                json.dump(summary, f, indent=2, default=str)\n            logger.info(f\"ğŸ’¾ Results saved to {results_file}\")\n        except:\n            pass\n        \n        return summary\n\n# ============================================================================\n# PIPELINE EXECUTION\n# ============================================================================\n\nlogger.info(\"=\"*80)\nlogger.info(\"ğŸš€ INITIALIZING MULTI-DOMAIN RAG PIPELINE\")\nlogger.info(\"=\"*80)\n\nrag_pipeline = MultiDomainRAGPipeline(config, DOMAINS)\n\ntest_queries = [\n    \"What are the recommended health screenings for women in their 40s?\",\n    \"Explain the symptoms and management of preeclampsia.\",\n    \"What are the early warning signs of Parkinson's disease?\",\n    \"How is PCOS diagnosed and treated?\",\n    \"What are the differences between Type 1 and Type 2 diabetes?\"\n]\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"ğŸ§ª RUNNING SINGLE QUERY TEST WITH DIAGNOSTICS\")\nlogger.info(\"=\"*80)\n\ntest_query = test_queries[0]\nresult = rag_pipeline.run_query(test_query, top_domains=2, log_diagnostics=True)\n\nlogger.info(f\"\\n{'='*80}\")\nlogger.info(f\"ğŸ“‹ QUERY: {result['query']}\")\nlogger.info(f\"{'='*80}\")\nlogger.info(f\"ğŸ¯ Routed to: {result['routed_domains']}\")\nlogger.info(f\"\\nâœ… ANSWER:\\n{result['answer']}\")\nlogger.info(f\"\\nğŸ“Š METRICS:\")\nfor metric_name, value in result['metrics'].items():\n    logger.info(f\"   {metric_name}: {value:.3f}\")\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"ğŸ“Š RUNNING BATCH EVALUATION\")\nlogger.info(\"=\"*80)\n\nbatch_results = rag_pipeline.evaluate_batch(test_queries[:3], log_diagnostics=False)\n\nlogger.info(f\"\\n{'='*80}\")\nlogger.info(\"ğŸ“ˆ BATCH EVALUATION SUMMARY\")\nlogger.info(f\"{'='*80}\")\nlogger.info(f\"Success Rate: {batch_results['success_rate']:.1%}\")\nlogger.info(f\"Average Retrieval: {batch_results['average_metrics']['retrieval']:.3f}\")\nlogger.info(f\"Average Completeness: {batch_results['average_metrics']['completeness']:.3f}\")\nlogger.info(f\"Average Faithfulness: {batch_results['average_metrics']['faithfulness']:.3f}\")\nlogger.info(f\"Average Composite: {batch_results['average_metrics']['composite']:.3f}\")\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"âœ… MULTI-DOMAIN RAG PIPELINE COMPLETE\")\nlogger.info(\"=\"*80)\n\ntry:\n    with open(config.prompts_log, \"wb\") as f:\n        pickle.dump(rag_pipeline.prompts_log, f)\n    logger.info(f\"ğŸ“ Prompt logs saved to {config.prompts_log}\")\nexcept:\n    pass\n\nmonitor_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T18:21:39.690143Z","iopub.execute_input":"2025-10-24T18:21:39.690663Z","iopub.status.idle":"2025-10-24T18:22:03.456639Z","shell.execute_reply.started":"2025-10-24T18:21:39.690643Z","shell.execute_reply":"2025-10-24T18:22:03.455576Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3a774469fa402592e13eba1634d324"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e580b67efb4761848385da6a97c213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c396be5a0c4f1eb0440a65da9e80cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"073f009e03624c6ea767e04c3815e80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf650bc1fa846a9856870487523328f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9d9abb36e74a94a1c9492a62e73d95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"069c0b7feeb64ff58056385e5e841f44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf149cc38333435e8258b57c19bd33fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd09280834574384b1e2376a06c012c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd50796b60746f79598102a8932445f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfb6ae73403c4b059595498ca36245a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3423f97bc4944bd59ca1f3bc86342e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843c379d89c0476fa5ad9a3942b891ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82436277dc0b438e95b1461999d7adf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2897e08297f4b1c82ddc3996b55ecad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fcb2cc5673a47beb3e0b8061ee4b97d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb5a717ebdb14ade87993aff67a29205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7602df3a3d6c4de2a886472ebf078152"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e76b34ab064297a9995d5f3813d2ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d46612827d4895ad5d922389c61413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e53e25c980144eeac0b84d96b0172b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c41355ba4674c108ac78edeb0b288c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9521ec64582d40629c63d019a2ad27d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df3f58317f5f40f080529312304863d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b0ae4928d24ed1a41708146ec03e45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdeaef70a6cd4dc292beb4f3c99436b6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_194/45142716.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m \u001b[0mrag_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiDomainRAGPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOMAINS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m test_queries = [\n","\u001b[0;32m/tmp/ipykernel_194/45142716.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, domains)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_194/45142716.py\u001b[0m in \u001b[0;36mload_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_reranker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_hyde_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_keyword_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mmonitor_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_194/45142716.py\u001b[0m in \u001b[0;36mload_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ“¦ Loading generator...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n\u001b[0;32m--> 603\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconH1Config, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config."],"ename":"ValueError","evalue":"Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconH1Config, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}